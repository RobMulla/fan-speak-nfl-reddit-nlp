{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with Attention Keras\n",
    "Rob Trying to make a LSTM with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Bears_vs_Packers__2017-09-28_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Broncos_vs_Chiefs__2017-10-30_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Chargers_vs_Cowboys__2017-11-23_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Chiefs_vs_Patriots__2017-09-07_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Chiefs_vs_Raiders__2017-10-19_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Cowboys_vs_Cardinals__2017-09-25_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Cowboys_vs_Raiders__2017-12-17_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Eagles_vs_Panthers__2017-10-12_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Falcons_vs_Buccaneers__2017-12-18_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Falcons_vs_Patriots__2017-10-22_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Falcons_vs_Seahawks__2017-11-20_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Giants_vs_Cowboys__2017-09-10_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Jaguars_vs_Patriots__2018-01-21_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Lions_vs_Giants__2017-09-18_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Lions_vs_Packers__2017-11-06_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Packers_vs_Panthers__2017-12-17_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Packers_vs_Vikings__2017-10-15_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Patriots_vs_Dolphins__2017-12-11_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Raiders_vs_Eagles__2017-12-25_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Raiders_vs_Redskins__2017-09-24_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Rams_vs_49ers__2017-09-21_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Redskins_vs_Chiefs__2017-10-02_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Redskins_vs_Cowboys__2017-11-30_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Redskins_vs_Eagles__2017-10-23_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Saints_vs_Falcons__2017-12-07_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Saints_vs_Vikings__2017-09-11_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Seahawks_vs_Cardinals__2017-11-09_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Steelers_vs_Bengals__2017-12-04_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Steelers_vs_Lions__2017-10-29_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Texans_vs_Bengals__2017-09-14_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Vikings_vs_Packers__2017-12-23_comment_sentiment.pickle\n",
      "/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/Vikings_vs_Panthers__2017-12-10_comment_sentiment.pickle\n"
     ]
    }
   ],
   "source": [
    "## Load comments by game\n",
    "files = [\n",
    "    'Bears_vs_Packers__2017-09-28_comment_sentiment.pickle',\n",
    "    'Broncos_vs_Chiefs__2017-10-30_comment_sentiment.pickle',\n",
    "    'Chargers_vs_Cowboys__2017-11-23_comment_sentiment.pickle',\n",
    "    'Chiefs_vs_Patriots__2017-09-07_comment_sentiment.pickle',\n",
    "    'Chiefs_vs_Raiders__2017-10-19_comment_sentiment.pickle',\n",
    "    'Cowboys_vs_Cardinals__2017-09-25_comment_sentiment.pickle',\n",
    "    'Cowboys_vs_Raiders__2017-12-17_comment_sentiment.pickle',\n",
    "    'Eagles_vs_Panthers__2017-10-12_comment_sentiment.pickle',\n",
    "    'Falcons_vs_Buccaneers__2017-12-18_comment_sentiment.pickle',\n",
    "    'Falcons_vs_Patriots__2017-10-22_comment_sentiment.pickle',\n",
    "    'Falcons_vs_Seahawks__2017-11-20_comment_sentiment.pickle',\n",
    "    'Giants_vs_Cowboys__2017-09-10_comment_sentiment.pickle',\n",
    "    'Jaguars_vs_Patriots__2018-01-21_comment_sentiment.pickle',\n",
    "    'Lions_vs_Giants__2017-09-18_comment_sentiment.pickle',\n",
    "    'Lions_vs_Packers__2017-11-06_comment_sentiment.pickle',\n",
    "    'Packers_vs_Panthers__2017-12-17_comment_sentiment.pickle',\n",
    "    'Packers_vs_Vikings__2017-10-15_comment_sentiment.pickle',\n",
    "    'Patriots_vs_Dolphins__2017-12-11_comment_sentiment.pickle',\n",
    "    'Raiders_vs_Eagles__2017-12-25_comment_sentiment.pickle',\n",
    "    'Raiders_vs_Redskins__2017-09-24_comment_sentiment.pickle',\n",
    "    'Rams_vs_49ers__2017-09-21_comment_sentiment.pickle',\n",
    "    'Redskins_vs_Chiefs__2017-10-02_comment_sentiment.pickle',\n",
    "    'Redskins_vs_Cowboys__2017-11-30_comment_sentiment.pickle',\n",
    "    'Redskins_vs_Eagles__2017-10-23_comment_sentiment.pickle',\n",
    "    'Saints_vs_Falcons__2017-12-07_comment_sentiment.pickle',\n",
    "    'Saints_vs_Vikings__2017-09-11_comment_sentiment.pickle',\n",
    "    'Seahawks_vs_Cardinals__2017-11-09_comment_sentiment.pickle',\n",
    "    'Steelers_vs_Bengals__2017-12-04_comment_sentiment.pickle',\n",
    "    'Steelers_vs_Lions__2017-10-29_comment_sentiment.pickle',\n",
    "    'Texans_vs_Bengals__2017-09-14_comment_sentiment.pickle',\n",
    "    'Vikings_vs_Packers__2017-12-23_comment_sentiment.pickle',\n",
    "    'Vikings_vs_Panthers__2017-12-10_comment_sentiment.pickle']\n",
    "\n",
    "path = \"/home/robmulla/Documents/W266/w266_final_project_merged/data/Clean_Game_Data/\"\n",
    "\n",
    "for index, filename in enumerate(files):\n",
    "    print(path+filename)\n",
    "    if index == 0:\n",
    "        data = pd.read_pickle(path+filename)\n",
    "        # print(data.head())\n",
    "    else:\n",
    "        temp_data = pd.read_pickle(path+filename)\n",
    "        data = data.append(temp_data)\n",
    "        # print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8784                                Fisher you fat fuck! \n",
       "8814    Hmm, looks like a bad spot, I think Smith got ...\n",
       "8825                           Yea, first down for sure. \n",
       "8843    God damn, our o-line is just terrible at run b...\n",
       "8860      Part of it is injuries to our interior o-line. \n",
       "Name: comment_body, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.author == 'Scaryclouds'].comment_body.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(588378, 43)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211168, 43)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fans = data.loc[(data['author_flair'] == data['home_team']) | (data['author_flair'] == data['away_team'])]\n",
    "data_fans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225254, 43)\n",
      "(153364, 43)\n"
     ]
    }
   ],
   "source": [
    "def gametype(row):\n",
    "    if (row > 0.9) | (row < 0.1):\n",
    "        return 'Blowout'\n",
    "    elif (row > 0.4) & (row < 0.6):\n",
    "        return 'Close'\n",
    "    else:\n",
    "        return 'Middleground'\n",
    "\n",
    "data['game_suspense'] = data['homeWinPercentage'].apply(gametype)\n",
    "print(data['game_suspense'.)\n",
    "print(data_close.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "game_suspense\n",
       "Blowout         170925\n",
       "Close           114181\n",
       "Middleground    303272\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('game_suspense').count()['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_close_blowout = data.loc[data['game_suspense'] != 'Middleground'].copy()\n",
    "\n",
    "data_close_blowout['game_suspense_binary'] = data_close_blowout['game_suspense'].apply(lambda x: 1 if x =='Blowout' else 0)\n",
    "\n",
    "labels = data_close_blowout['game_suspense_binary'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "docs = data_close_blowout['comment_body']\n",
    "# Lowercase words\n",
    "X_data = docs.map(lambda x: x.lower())\n",
    "\n",
    "# Convert links to word 'postedhyperlinkvalue'\n",
    "X_data = X_data.map(lambda x: re.sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)*\\/?\\S\", \\\n",
    "                                     \"postedhyperlinkvalue\", x))\n",
    "\n",
    "# Convert digits to 'd'\n",
    "X_data = X_data.map(lambda x: re.sub('\\d', 'd', x))\n",
    "\n",
    "# Tokenize words\n",
    "X_data = X_data.map(lambda x: word_tokenize(x))\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "X_data = X_data.map(lambda x: [w for w in x if not w in stop_words])\n",
    "\n",
    "# Word stemming\n",
    "\n",
    "ps = PorterStemmer()\n",
    "X_data = X_data.map(lambda x: [ps.stem(w) for w in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100000\n",
    "encoded_docs = [one_hot(str(d), vocab_size) for d in X_data]\n",
    "\n",
    "# pad documents to a max length of 500 words\n",
    "max_length = 100\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 8)            800000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               43600     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 843,701\n",
      "Trainable params: 843,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 191021 samples, validate on 94085 samples\n",
      "Epoch 1/3\n",
      "191021/191021 [==============================] - 260s 1ms/step - loss: 0.6523 - acc: 0.6428 - val_loss: 0.7265 - val_acc: 0.5116\n",
      "Epoch 2/3\n",
      "191021/191021 [==============================] - 260s 1ms/step - loss: 0.6520 - acc: 0.6428 - val_loss: 0.7229 - val_acc: 0.5116\n",
      "Epoch 3/3\n",
      "191021/191021 [==============================] - 258s 1ms/step - loss: 0.6519 - acc: 0.6428 - val_loss: 0.7253 - val_acc: 0.5116\n",
      "285106/285106 [==============================] - 71s 251us/step\n",
      "Accuracy: 59.951387\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=3, verbose=1, validation_split=0.33)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "docs = data_close_blowout['comment_body']\n",
    "# Lowercase words\n",
    "X_data = docs.map(lambda x: x.lower())\n",
    "\n",
    "# Convert links to word 'postedhyperlinkvalue'\n",
    "X_data = X_data.map(lambda x: re.sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)*\\/?\\S\", \\\n",
    "                                     \"postedhyperlinkvalue\", x))\n",
    "\n",
    "# Convert digits to 'd'\n",
    "X_data = X_data.map(lambda x: re.sub('\\d', 'd', x))\n",
    "\n",
    "# Tokenize words\n",
    "X_data = X_data.map(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 8)            800000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               43600     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 843,701\n",
      "Trainable params: 843,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 191021 samples, validate on 94085 samples\n",
      "Epoch 1/3\n",
      "191021/191021 [==============================] - 289s 2ms/step - loss: 0.6523 - acc: 0.6428 - val_loss: 0.7228 - val_acc: 0.5116\n",
      "Epoch 2/3\n",
      "191021/191021 [==============================] - 289s 2ms/step - loss: 0.6521 - acc: 0.6428 - val_loss: 0.7237 - val_acc: 0.5116\n",
      "Epoch 3/3\n",
      "191021/191021 [==============================] - 299s 2ms/step - loss: 0.6518 - acc: 0.6428 - val_loss: 0.7258 - val_acc: 0.5116\n",
      "285106/285106 [==============================] - 79s 277us/step\n",
      "Accuracy: 59.951387\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 100000\n",
    "encoded_docs = [one_hot(str(d), vocab_size) for d in X_data]\n",
    "\n",
    "# pad documents to a max length of 500 words\n",
    "max_length = 100\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=3, verbose=1, validation_split=0.33)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
