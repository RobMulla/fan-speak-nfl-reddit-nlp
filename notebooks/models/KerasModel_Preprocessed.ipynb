{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model CNN for Classification\n",
    "## April 8, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "import gensim\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Conv1D, MaxPooling1D , Dense, Dropout, Activation, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# NLTK libs\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../../Comments_FanofGame_data_bandwagon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author_flair</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>fan_of_team_playing</th>\n",
       "      <th>bandwagon</th>\n",
       "      <th>fan_of_team_playing_w_bandwagon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bengals</td>\n",
       "      <td>Game Thread: Detroit Lions (8-6) at Cincinnati...</td>\n",
       "      <td>I love Lions fans cause we can drink bleach to...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lions</td>\n",
       "      <td>Game Thread: Detroit Lions (8-6) at Cincinnati...</td>\n",
       "      <td>I was back and forth with this, but I'm on the...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Lions</td>\n",
       "      <td>Game Thread: Detroit Lions (8-6) at Cincinnati...</td>\n",
       "      <td>Ebron with that sick Naruto run.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Lions</td>\n",
       "      <td>Game Thread: Detroit Lions (8-6) at Cincinnati...</td>\n",
       "      <td>\"Here, Merry Christmas. have this win.\"\\n\\n\"No...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Eagles - Bandwagon</td>\n",
       "      <td>Game Thread: Detroit Lions (8-6) at Cincinnati...</td>\n",
       "      <td>The coaching staff does realize that if we los...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        author_flair  \\\n",
       "0           0             Bengals   \n",
       "1           1               Lions   \n",
       "2           2               Lions   \n",
       "3           3               Lions   \n",
       "4           4  Eagles - Bandwagon   \n",
       "\n",
       "                                    submission_title  \\\n",
       "0  Game Thread: Detroit Lions (8-6) at Cincinnati...   \n",
       "1  Game Thread: Detroit Lions (8-6) at Cincinnati...   \n",
       "2  Game Thread: Detroit Lions (8-6) at Cincinnati...   \n",
       "3  Game Thread: Detroit Lions (8-6) at Cincinnati...   \n",
       "4  Game Thread: Detroit Lions (8-6) at Cincinnati...   \n",
       "\n",
       "                                        comment_body  fan_of_team_playing  \\\n",
       "0  I love Lions fans cause we can drink bleach to...                    1   \n",
       "1  I was back and forth with this, but I'm on the...                    1   \n",
       "2                  Ebron with that sick Naruto run.                     1   \n",
       "3  \"Here, Merry Christmas. have this win.\"\\n\\n\"No...                    1   \n",
       "4  The coaching staff does realize that if we los...                    0   \n",
       "\n",
       "   bandwagon  fan_of_team_playing_w_bandwagon  \n",
       "0          0                                1  \n",
       "1          0                                1  \n",
       "2          0                                1  \n",
       "3          0                                1  \n",
       "4          1                                0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                         1542039\n",
       "author_flair                       1542039\n",
       "submission_title                   1542039\n",
       "comment_body                       1542039\n",
       "fan_of_team_playing                1542039\n",
       "bandwagon                          1542039\n",
       "fan_of_team_playing_w_bandwagon    1542039\n",
       "fan_binary                         1542039\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove problematic comments\n",
    "data = data[data.comment_body.notnull()]\n",
    "data = data[data.comment_body != \"no value\"]\n",
    "data = data[data.comment_body!='[removed]']\n",
    "data = data[data.comment_body!='[deleted]']\n",
    "data = data[~data.comment_body.str.contains(\"\\^This \\^message \\^was \\^created \\^by \\^a \\^bot\")]\n",
    "data = data[~data.comment_body.str.contains(\"\\*\\*Please review the rules for\")]\n",
    "data = data[data.notnull()]\n",
    "\n",
    "\n",
    "# Remove Superbowl and playoff Threads\n",
    "data = data[\n",
    "(data.submission_title!='Super Bowl LII Game Thread: Philadelphia Eagles (15-3) at New England Patriots (15-3)') &\n",
    "(data.submission_title!='Super Bowl LII Game Thread: Philadelphia Eagles (15-3) at New England Patriots (15-3) (Second Half)') &\n",
    "(data.submission_title!='Super Bowl 51 Pre Game Thread: New England Patriots (14-2) at Atlanta Falcons (11-5)') &\n",
    "(data.submission_title!='Super Bowl 51 Post Game Thread: New England Patriots (14-2) at Atlanta Falcons (11-5)') &\n",
    "(data.submission_title!='Game Thread: Atlanta Falcons (10-6) at Los Angeles Rams (11-5)') &\n",
    "(data.submission_title!='Game Thread: Tennessee Titans (9-7) at Kansas City Chiefs (10-6)') &\n",
    "(data.submission_title!='Game Thread: Buffalo Bills (9-7) at Jacksonville Jaguars (10-6)') &\n",
    "(data.submission_title!='Game Thread: Carolina Panthers (11-5) at New Orleans Saints (11-5)') &\n",
    "(data.submission_title!='Game Thread: Atlanta Falcons (11-6) at Philadelphia Eagles (13-3)') &\n",
    "(data.submission_title!='Game Thread: Tennessee Titans (10-7) at New England Patriots (13-3)') &\n",
    "(data.submission_title!='Game Thread: Jacksonville Jaguars (11-6) at Pittsburgh Steelers (13-3)') &\n",
    "(data.submission_title!='Game Thread: Jacksonville Jaguars (12-6) at New England Patriots (14-3)') &\n",
    "(data.submission_title!='Game Thread: Minnesota Vikings (14-3) at Philadelphia Eagles (14-3)') \n",
    "]\n",
    "\n",
    "data['fan_binary'] = data.fan_of_team_playing.astype(int)\n",
    "\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed some functions from the w266 utils.py file\n",
    "# Miscellaneous helpers\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"Flatten a list-of-lists into a single list.\"\"\"\n",
    "    return list(itertools.chain.from_iterable(list_of_lists))\n",
    "\n",
    "\n",
    "# Word processing functions\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    #replace hyperlinks with one instance of \"postedhyperlinkvalue\"\n",
    "    word = re.sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)*\\/?\\S*\", \"postedhyperlinkvalue\", word)\n",
    "    word = re.sub(r\"(postedhyperlinkvalue)+\", \"postedhyperlinkvalue\", word)\n",
    "    #only lower case words (2 letters or longer) that are not all upper case\n",
    "    if not word.isupper() or len(word) == 1:\n",
    "        word = word.lower()\n",
    "    #replace things like haha with ha\n",
    "    word = re.sub(r\"([a-z]{2,})\\1{2,}\", r\"\\1\", word)\n",
    "    #replace any three consecutive, identical letters with two instances of that letter\n",
    "    word = re.sub(r\"([a-z])\\1{2,}\", r\"\\1\\1\", word)\n",
    "    #replace any two consecutive, identical consonants at the beginning of a string with one of that consonant\n",
    "    word = re.sub(r\"(^[^aeiou])\\1{1,}\", r\"\\1\", word)\n",
    "    \n",
    "    #replace digits with a stand-in token\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset):\n",
    "        return word\n",
    "    else:\n",
    "        return constants.UNK_TOKEN\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "    \n",
    "def stem_sentence(token_sent, stemmer=PorterStemmer()):\n",
    "    stem_token_sent = []\n",
    "    for word in token_sent:\n",
    "        stem_token_sent.append(stemmer.stem(word))\n",
    "    return stem_token_sent   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(data, target, tokenizer=TweetTokenizer(), canonize=True, stem=True):\n",
    "        \n",
    "    # Separate comments\n",
    "    comments = data.loc[:, 'comment_body']\n",
    "    labels = data.loc[:, target]\n",
    "    \n",
    "    # Convert to list\n",
    "    comment_list = comments.values.tolist()\n",
    "    \n",
    "    # Tokenize comments\n",
    "    tokenizer = tokenizer\n",
    "    # A list of lists of tokenized sentences: word == string/token; sentence == list of string/tokens\n",
    "    tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in comment_list]\n",
    "    \n",
    "    if stem:\n",
    "        # Stem words\n",
    "        comments_stem = []\n",
    "        for sentence in tokenized_sentences:\n",
    "            x_tokens_stem = stem_sentence(token_sent=sentence, stemmer=PorterStemmer())\n",
    "            comments_stem.append(x_tokens_stem)\n",
    "        tokenized_sentences = comments_stem\n",
    "    \n",
    "    if canonize:\n",
    "        # Canonize words\n",
    "        comments_canon = []\n",
    "        for sentence in tokenized_sentences:\n",
    "            x_tokens_canon = canonicalize_words(sentence)\n",
    "            comments_canon.append(x_tokens_canon)\n",
    "        # A list of lists of scrubbed tokens; token == word, list == sentence\n",
    "        tokenized_sentences = comments_canon\n",
    "    \n",
    "    x_tokens = tokenized_sentences\n",
    "        \n",
    "    return comments, x_tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments, x_tokens, labels = make_data(data, target='fan_of_team_playing', canonize=True, stem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I love Lions fans cause we can drink bleach to...\n",
       "1    I was back and forth with this, but I'm on the...\n",
       "2                    Ebron with that sick Naruto run. \n",
       "Name: comment_body, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'hope',\n",
       "  'so',\n",
       "  '.',\n",
       "  'the',\n",
       "  'kid',\n",
       "  'has',\n",
       "  'looked',\n",
       "  'very',\n",
       "  'sharp',\n",
       "  ',',\n",
       "  'better',\n",
       "  'than',\n",
       "  'fitztragic',\n",
       "  '.'],\n",
       " ['it',\n",
       "  'really',\n",
       "  'was',\n",
       "  '.',\n",
       "  'one',\n",
       "  'of',\n",
       "  'those',\n",
       "  'catches',\n",
       "  'that',\n",
       "  'just',\n",
       "  'makes',\n",
       "  'you',\n",
       "  'say',\n",
       "  '\"',\n",
       "  'how',\n",
       "  'the',\n",
       "  'hell',\n",
       "  'did',\n",
       "  'that',\n",
       "  'even',\n",
       "  'happen',\n",
       "  '?',\n",
       "  '!',\n",
       "  '\"'],\n",
       " [\"there's\",\n",
       "  'lack',\n",
       "  'of',\n",
       "  'talent',\n",
       "  'and',\n",
       "  'bad',\n",
       "  'scheme',\n",
       "  '(',\n",
       "  'defense',\n",
       "  ')',\n",
       "  'and',\n",
       "  'then',\n",
       "  \"there's\",\n",
       "  'lack',\n",
       "  'of',\n",
       "  'polish',\n",
       "  '(',\n",
       "  'special',\n",
       "  'teams',\n",
       "  ')',\n",
       "  '.',\n",
       "  'i',\n",
       "  'would',\n",
       "  'point',\n",
       "  'out',\n",
       "  'that',\n",
       "  'the',\n",
       "  'defense',\n",
       "  'has',\n",
       "  'also',\n",
       "  'had',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'penalties',\n",
       "  '.'],\n",
       " [\"it's\",\n",
       "  'very',\n",
       "  'clear',\n",
       "  'watching',\n",
       "  'them',\n",
       "  ',',\n",
       "  'so',\n",
       "  'far',\n",
       "  ',',\n",
       "  'that',\n",
       "  'our',\n",
       "  'last',\n",
       "  'two',\n",
       "  'drafts',\n",
       "  'have',\n",
       "  'really',\n",
       "  'transformed',\n",
       "  'this',\n",
       "  'team',\n",
       "  '.',\n",
       "  'we',\n",
       "  'have',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'very',\n",
       "  'young',\n",
       "  'talent',\n",
       "  '.',\n",
       "  'importantly',\n",
       "  ',',\n",
       "  'few',\n",
       "  'busts',\n",
       "  '.',\n",
       "  '\"',\n",
       "  'moneyball',\n",
       "  '\"',\n",
       "  'appears',\n",
       "  'to',\n",
       "  'be',\n",
       "  'succeeding',\n",
       "  'in',\n",
       "  'cleveland',\n",
       "  '.',\n",
       "  'feels',\n",
       "  'good',\n",
       "  '.'],\n",
       " ['only',\n",
       "  'when',\n",
       "  'someone',\n",
       "  'trash',\n",
       "  'talks',\n",
       "  'back',\n",
       "  '.',\n",
       "  'i',\n",
       "  \"don't\",\n",
       "  'go',\n",
       "  'for',\n",
       "  'the',\n",
       "  'browns',\n",
       "  ',',\n",
       "  'you',\n",
       "  'guys',\n",
       "  'have',\n",
       "  'been',\n",
       "  'through',\n",
       "  'too',\n",
       "  'much',\n",
       "  'shit',\n",
       "  'and',\n",
       "  'get',\n",
       "  'too',\n",
       "  'much',\n",
       "  'flack',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'argument',\n",
       "  \"isn't\",\n",
       "  'for',\n",
       "  'schedule',\n",
       "  ',',\n",
       "  'as',\n",
       "  'we',\n",
       "  'played',\n",
       "  'some',\n",
       "  'high-scoring',\n",
       "  'teams',\n",
       "  'in',\n",
       "  'our',\n",
       "  'second',\n",
       "  'half',\n",
       "  '.',\n",
       "  'the',\n",
       "  'argument',\n",
       "  'would',\n",
       "  'be',\n",
       "  'for',\n",
       "  'pace',\n",
       "  'of',\n",
       "  'play',\n",
       "  ',',\n",
       "  'which',\n",
       "  'skews',\n",
       "  'scoring',\n",
       "  'stats',\n",
       "  '-',\n",
       "  '-',\n",
       "  'versus',\n",
       "  'points',\n",
       "  'allowed',\n",
       "  'per',\n",
       "  'possession',\n",
       "  ',',\n",
       "  'where',\n",
       "  'we',\n",
       "  'were',\n",
       "  'only',\n",
       "  'top',\n",
       "  'DGDG',\n",
       "  'in',\n",
       "  'the',\n",
       "  'second',\n",
       "  'half',\n",
       "  '.',\n",
       "  'regardless',\n",
       "  ',',\n",
       "  'the',\n",
       "  'claim',\n",
       "  'that',\n",
       "  'tampa',\n",
       "  'had',\n",
       "  'an',\n",
       "  'awful',\n",
       "  'defense',\n",
       "  'last',\n",
       "  'year',\n",
       "  'is',\n",
       "  'just',\n",
       "  'wrong',\n",
       "  'by',\n",
       "  'nearly',\n",
       "  'any',\n",
       "  'metric',\n",
       "  '.',\n",
       "  'we',\n",
       "  'were',\n",
       "  'somewhere',\n",
       "  'between',\n",
       "  'average',\n",
       "  'and',\n",
       "  'below',\n",
       "  'average',\n",
       "  '.'],\n",
       " ['it',\n",
       "  'would',\n",
       "  'actually',\n",
       "  'be',\n",
       "  'DG-DG-',\n",
       "  'DGDG',\n",
       "  'since',\n",
       "  \"they'd\",\n",
       "  'have',\n",
       "  'to',\n",
       "  'play',\n",
       "  'the',\n",
       "  'wild',\n",
       "  'card',\n",
       "  'round',\n",
       "  '.',\n",
       "  'theres',\n",
       "  'not',\n",
       "  'mathematically',\n",
       "  'possibly',\n",
       "  'of',\n",
       "  'a',\n",
       "  'DGDG',\n",
       "  'tie',\n",
       "  'team',\n",
       "  'being',\n",
       "  'the',\n",
       "  '#',\n",
       "  'DG',\n",
       "  'or',\n",
       "  'even',\n",
       "  '#',\n",
       "  'DG',\n",
       "  'seed',\n",
       "  '.',\n",
       "  'but',\n",
       "  'im',\n",
       "  'not',\n",
       "  'an',\n",
       "  'applied',\n",
       "  'math',\n",
       "  'major',\n",
       "  'so',\n",
       "  'someone',\n",
       "  'correct',\n",
       "  'me',\n",
       "  'if',\n",
       "  'im',\n",
       "  'wrong',\n",
       "  '.'],\n",
       " ['another', 'punt', ',', 'another', 'flag', '.', 'just', 'saying', '.'],\n",
       " ['so',\n",
       "  'a',\n",
       "  'guy',\n",
       "  'held',\n",
       "  'unnecessarily',\n",
       "  'on',\n",
       "  'a',\n",
       "  'non',\n",
       "  'return',\n",
       "  ',',\n",
       "  'hard',\n",
       "  'to',\n",
       "  'say',\n",
       "  \"that's\",\n",
       "  'the',\n",
       "  \"coach's\",\n",
       "  'fault']]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tokens[1542030:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "Name: fan_of_team_playing, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word2vec embeddings model (skip-gram w/negative sampling)\n",
    "vector_dim = 100\n",
    "model_nsg_pp2 = gensim.models.Word2Vec(sentences=x_tokens, sg=1, size=vector_dim, hs=0, negative=12, workers=8, compute_loss=True, max_vocab_size=None, min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_nsg_pp2.save('model_nsg_pp2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can easily load the model like this:\n",
    "model_nsg_pp2 = gensim.models.Word2Vec.load('model_nsg_pp2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=134884, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the embeddings\n",
    "print(model_nsg_pp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':/', 0.9012995958328247), (\":'(\", 0.8864564895629883), ('=(', 0.8806827068328857), (':-(', 0.8279445171356201), (';(', 0.8038519024848938), ('😣', 0.793906569480896), ('fml', 0.7796511054039001), (':)', 0.771406888961792), ('.', 0.7657585144042969), (':-/', 0.7642029523849487)]\n",
      "[('lmao', 0.9632657170295715), ('lmfao', 0.9145869016647339), ('haha', 0.9018850326538086), ('lmaoo', 0.8737468719482422), ('rofl', 0.8569039106369019), ('loll', 0.8550130128860474), ('hahah', 0.8459514379501343), ('lolol', 0.8391028046607971), ('bahaha', 0.8385341167449951), ('lmfaoo', 0.8384135961532593)]\n",
      "[('TD', 0.9414183497428894), ('td', 0.9301056861877441), ('tuddy', 0.8021426200866699), ('interception', 0.7971596121788025), ('turnover', 0.7834737300872803), ('fumble', 0.7796392440795898), ('touchdowns', 0.7755153179168701), ('freebie', 0.7647457122802734), ('strip-sack', 0.7621861696243286), ('INT', 0.7490881085395813)]\n",
      "[('hater', 0.801754355430603), ('lifelong', 0.783571720123291), ('fans', 0.7712509632110596), (\"fan's\", 0.7710649371147156), ('diehard', 0.7648425698280334), ('watcher', 0.7535024881362915), ('resident', 0.7434656620025635), ('realist', 0.7330871820449829), ('spectator', 0.7267471551895142), ('longhorn', 0.7205253839492798)]\n",
      "[('ref', 0.8288551568984985), ('officials', 0.7907470464706421), ('referees', 0.7478832602500916), ('grounds', 0.7468515038490295), ('official', 0.7298185229301453), ('umpire', 0.7221977710723877), ('morelli', 0.7124854326248169), (\"referee's\", 0.7035280466079712), ('triplette', 0.6952153444290161), ('officiating', 0.6885654926300049)]\n",
      "[('fk', 0.8279156684875488), ('hell', 0.8258254528045654), ('fook', 0.7652411460876465), ('fck', 0.7574689984321594), ('fuckkkk', 0.7547856569290161), ('#fuck', 0.7436493635177612), ('fukk', 0.7404550313949585), ('heck', 0.7393157482147217), ('eff', 0.7293696403503418), ('fuckkk', 0.7266114950180054)]\n"
     ]
    }
   ],
   "source": [
    "# Look at example word similarities WARNING: SOME INAPPROPRIATE WORDS MAY BE DISPLAYED\n",
    "print(model_nsg_pp2.wv.most_similar(':(')) # it can handle emoticons\n",
    "#print(model_nsg_pp.wv.most_similar('Bengals')) # ok, i expect the steelers but the browns...\n",
    "print(model_nsg_pp2.wv.most_similar('lol')) # it can handle shorthand\n",
    "print(model_nsg_pp2.wv.most_similar('touchdown')) # makes sense\n",
    "print(model_nsg_pp2.wv.most_similar('fan')) # well, that's a mixed bag\n",
    "print(model_nsg_pp.wv.most_similar('referee')) # actually identifies some refs by name\n",
    "print(model_nsg_pp.wv.most_similar('fuck')) # actually identifies some refs by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.09258407e-01  4.57670689e-01  1.49294421e-01 -1.83503330e-01\n",
      "  3.69378954e-01  9.88171324e-02 -1.82675496e-01 -2.47694314e-01\n",
      " -1.34315297e-01  3.04550409e-01  3.64713013e-01  2.84847736e-01\n",
      " -2.19126791e-01 -2.42191046e-01  1.69074103e-01 -8.37326124e-02\n",
      " -1.04633801e-01  1.14765622e-01 -4.44723433e-03 -8.76004249e-02\n",
      "  1.38789177e-01 -1.08933486e-01 -3.29811834e-02  2.94851631e-01\n",
      "  3.79643261e-01  4.34226580e-02  6.99976925e-03  1.57083273e-01\n",
      " -4.82850611e-01  3.96976501e-01 -2.82388572e-02 -2.84985781e-01\n",
      " -2.80171424e-01 -2.37428620e-01 -1.26461387e-01 -2.51442939e-02\n",
      "  8.92624930e-02  6.27043322e-02  4.49121781e-02  7.35627040e-02\n",
      " -2.19537199e-01  2.84047663e-01  5.00574648e-01  2.60297269e-01\n",
      " -2.45424777e-01  9.09172744e-02 -2.35160753e-01  1.02110669e-01\n",
      "  3.94375324e-02 -1.55339921e-02 -1.26940414e-01 -4.55784321e-01\n",
      "  5.09705208e-02 -1.56209067e-01  9.89836827e-02 -3.59149039e-01\n",
      " -2.39171356e-01  1.77287087e-01  1.94557905e-01 -2.11717233e-01\n",
      "  2.72588164e-01 -5.67095160e-01  1.61009416e-01  1.52532101e-01\n",
      "  6.53189495e-02  1.88642308e-01  3.18420470e-01 -6.01808317e-02\n",
      " -3.02129276e-02  1.43011391e-01 -1.69462904e-01  7.75730237e-02\n",
      "  1.18226878e-01 -1.45163322e-02 -2.34249175e-01 -6.82457268e-01\n",
      " -3.85656238e-01 -3.93464446e-01  2.06182271e-01 -9.12924930e-02\n",
      "  6.05158806e-01 -9.52621475e-02 -1.15570729e-05  1.98342875e-01\n",
      "  7.28352666e-02 -7.00483680e-01  1.66708082e-01  9.19301882e-02\n",
      " -1.88714027e-01 -1.73538357e-01 -1.28219053e-01  9.70931053e-02\n",
      "  7.01815367e-01  8.94486085e-02  4.20911700e-01 -3.32232982e-01\n",
      " -6.24563806e-02 -5.00773966e-01  4.41658735e-01 -7.97549263e-02]\n"
     ]
    }
   ],
   "source": [
    "# get the word vector of \"the\"\n",
    "print(model_nsg_pp2.wv['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". the a\n"
     ]
    }
   ],
   "source": [
    "# get the most common words\n",
    "print(model_nsg_pp2.wv.index2word[0], model_nsg_pp2.wv.index2word[1], model_nsg_pp2.wv.index2word[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greco oranger spacebrowns\n"
     ]
    }
   ],
   "source": [
    "# get the least common words\n",
    "vocab_size = len(model_nsg_pp2.wv.vocab)\n",
    "print(model_nsg_pp2.wv.index2word[vocab_size - 1], model_nsg_pp2.wv.index2word[vocab_size - 2], model_nsg_pp2.wv.index2word[vocab_size - 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of \"i\" is: 5\n"
     ]
    }
   ],
   "source": [
    "# find the index of the word (\"i\")\n",
    "print('Index of \"i\" is: {}'.format(model_nsg_pp2.wv.vocab['i'].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref\n"
     ]
    }
   ],
   "source": [
    "# what doesn't fit?\n",
    "print(model_nsg_pp2.wv.doesnt_match(\"us we our ref\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the wv word vectors into a numpy matrix that is suitable for insertion\n",
    "# into our Keras model\n",
    "embedding_matrix = np.zeros((len(model_nsg_pp2.wv.vocab), vector_dim))\n",
    "for i in range(len(model_nsg_pp2.wv.vocab)):\n",
    "    embedding_vector = model_nsg_pp2.wv[model_nsg_pp2.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134884, 100)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 157, 362, 109, 611, 22, 54, 997, 4798, 858, 13, 953, 5226],\n",
       " [5,\n",
       "  16,\n",
       "  93,\n",
       "  9,\n",
       "  4144,\n",
       "  33,\n",
       "  13,\n",
       "  3,\n",
       "  26,\n",
       "  60,\n",
       "  17,\n",
       "  1,\n",
       "  564,\n",
       "  2528,\n",
       "  1620,\n",
       "  73,\n",
       "  0,\n",
       "  13,\n",
       "  7,\n",
       "  1363,\n",
       "  0],\n",
       " [1936, 33, 6, 959, 16895, 127, 0]]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs_w2v = [[model_nsg_pp2.wv.vocab[word].index for word in sentence] for sentence in x_tokens]\n",
    "padded_docs_w2v[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5280612228354795"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad documents to a max length of 100 words\n",
    "max_length = 100\n",
    "padded_docs = pad_sequences(padded_docs_w2v, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    5,   154,   363,   110,   609,    21,    56,   934,  4574,\n",
       "          844,    14,   930,  4972,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [    5,    17,    92,     9,  3983,    33,    14,     3,    26,\n",
       "           62,    16,     1,   519,  2418,  1462,    73,     0,    14,\n",
       "            6,  1361,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [ 1777,    33,     7,   945, 15027,   126,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0]], dtype=int32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 100, 100)          13488400  \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 100, 5)            505       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 98, 5)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 98, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 97, 10)            110       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 96, 10)            0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 96, 10)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 960)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               246016    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 13,768,185\n",
      "Trainable params: 279,785\n",
      "Non-trainable params: 13,488,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "num_filters = 5\n",
    "dropout=0.5\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, vector_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(Conv1D(num_filters,1,activation='tanh'))\n",
    "model.add(MaxPooling1D(pool_size=(3,),strides=1))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Conv1D(num_filters*2,2,activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2,),strides=1))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation='tanh'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(128,activation='tanh'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "#model.add(Activation('softmax'))\n",
    "#model.compile(loss=keras.losses.categorical_crossentropy,optimzer=keras.optimizers.SGD(),metrics=['accuracy'])\n",
    "adam = Adam(lr=0.0001, decay=1e-5)\n",
    "model.compile(loss='binary_crossentropy',optimizer=adam,metrics=['mse', 'acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1387835 samples, validate on 154204 samples\n",
      "Epoch 1/8\n",
      "1387835/1387835 [==============================] - 408s 294us/step - loss: 0.6853 - mean_squared_error: 0.2461 - acc: 0.5478 - val_loss: 0.6756 - val_mean_squared_error: 0.2414 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00001: saving model to weights-improvement-april15-cnn-pp-01-0.57.hdf5\n",
      "Epoch 2/8\n",
      "1387835/1387835 [==============================] - 411s 296us/step - loss: 0.6751 - mean_squared_error: 0.2412 - acc: 0.5700 - val_loss: 0.6705 - val_mean_squared_error: 0.2390 - val_acc: 0.5806\n",
      "\n",
      "Epoch 00002: saving model to weights-improvement-april15-cnn-pp-02-0.58.hdf5\n",
      "Epoch 3/8\n",
      "1387835/1387835 [==============================] - 418s 302us/step - loss: 0.6728 - mean_squared_error: 0.2401 - acc: 0.5750 - val_loss: 0.6689 - val_mean_squared_error: 0.2383 - val_acc: 0.5829\n",
      "\n",
      "Epoch 00003: saving model to weights-improvement-april15-cnn-pp-03-0.58.hdf5\n",
      "Epoch 4/8\n",
      "1387835/1387835 [==============================] - 424s 306us/step - loss: 0.6715 - mean_squared_error: 0.2395 - acc: 0.5773 - val_loss: 0.6686 - val_mean_squared_error: 0.2381 - val_acc: 0.5844\n",
      "\n",
      "Epoch 00004: saving model to weights-improvement-april15-cnn-pp-04-0.58.hdf5\n",
      "Epoch 5/8\n",
      "1387835/1387835 [==============================] - 428s 308us/step - loss: 0.6706 - mean_squared_error: 0.2391 - acc: 0.5792 - val_loss: 0.6679 - val_mean_squared_error: 0.2378 - val_acc: 0.5861\n",
      "\n",
      "Epoch 00005: saving model to weights-improvement-april15-cnn-pp-05-0.59.hdf5\n",
      "Epoch 6/8\n",
      "1387835/1387835 [==============================] - 382s 275us/step - loss: 0.6700 - mean_squared_error: 0.2388 - acc: 0.5801 - val_loss: 0.6675 - val_mean_squared_error: 0.2376 - val_acc: 0.5863\n",
      "\n",
      "Epoch 00006: saving model to weights-improvement-april15-cnn-pp-06-0.59.hdf5\n",
      "Epoch 7/8\n",
      "1387835/1387835 [==============================] - 374s 270us/step - loss: 0.6696 - mean_squared_error: 0.2386 - acc: 0.5808 - val_loss: 0.6670 - val_mean_squared_error: 0.2374 - val_acc: 0.5871\n",
      "\n",
      "Epoch 00007: saving model to weights-improvement-april15-cnn-pp-07-0.59.hdf5\n",
      "Epoch 8/8\n",
      "1387835/1387835 [==============================] - 370s 267us/step - loss: 0.6694 - mean_squared_error: 0.2385 - acc: 0.5808 - val_loss: 0.6670 - val_mean_squared_error: 0.2374 - val_acc: 0.5874\n",
      "\n",
      "Epoch 00008: saving model to weights-improvement-april15-cnn-pp-08-0.59.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b365a09e8>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint for periodically saving model\n",
    "filepath=\"weights-improvement-april15-cnn-pp-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=False, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "y_binary = to_categorical(labels)\n",
    "\n",
    "# fit the model\n",
    "model.fit(padded_docs, y_binary, epochs=8, verbose=1, validation_split=0.10, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 100, 100)          13488400  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 13,569,002\n",
      "Trainable params: 80,602\n",
      "Non-trainable params: 13,488,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#num_filters = 5\n",
    "dropout=0.2\n",
    "kernel_initializer=keras.initializers.RandomUniform(minval=-1, maxval=1, seed=None)\n",
    "recurrent_initializer=keras.initializers.Zeros()\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, vector_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(units=100, dropout=dropout, kernel_initializer=kernel_initializer, \n",
    "               recurrent_initializer=recurrent_initializer))\n",
    "model.add(Dense(2,activation='sigmoid'))\n",
    "adam = Adam(lr=0.0001, decay=1e-5)\n",
    "model.compile(loss='binary_crossentropy',optimizer=adam,metrics=['mse', 'acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1387835 samples, validate on 154204 samples\n",
      "Epoch 1/2\n",
      "1387835/1387835 [==============================] - 10293s 7ms/step - loss: 0.6920 - mean_squared_error: 0.2494 - acc: 0.5255 - val_loss: 0.6860 - val_mean_squared_error: 0.2464 - val_acc: 0.5564\n",
      "\n",
      "Epoch 00001: saving model to weights-improvement-april14-lstm-01-0.56.hdf5\n",
      "Epoch 2/2\n",
      "1387835/1387835 [==============================] - 9853s 7ms/step - loss: 0.6838 - mean_squared_error: 0.2453 - acc: 0.5565 - val_loss: 0.6758 - val_mean_squared_error: 0.2415 - val_acc: 0.5761\n",
      "\n",
      "Epoch 00002: saving model to weights-improvement-april14-lstm-02-0.58.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b34739a90>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint for periodically saving model\n",
    "filepath=\"weights-improvement-april14-lstm-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=False, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "y_binary = to_categorical(labels)\n",
    "\n",
    "# fit the model\n",
    "model.fit(padded_docs, y_binary, epochs=2, verbose=1, validation_split=0.10, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict On Dataset using Model\n",
    "- Identify Documents it predicted well and poorly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the same dataset\n",
    "predict_prob = model.predict(padded_docs)\n",
    "predictions = model.predict(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions alongside the comments in a dataframe\n",
    "model_preds = pd.DataFrame(index=docs)\n",
    "model_preds['predicted_class'] = 0\n",
    "model_preds['predicted_class'] = predictions\n",
    "# Predicted class is actually the inverse (I think)\n",
    "model_preds['predicted_class'] = 1 - model_preds['predicted_class']\n",
    "model_preds['actual_class'] = labels\n",
    "model_preds['prob_off'] = abs(model_preds['predicted_class'] - model_preds['actual_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f6d1580ab38>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f6d22900470>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f6d228a6780>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f6d228cca90>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot to see the distribution of the errors\n",
    "model_preds.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "----predicted as: 0.06995964050292969\n",
      "----actually: 0---(1 = fan, 0 = not fan --------\n",
      "-------comment-----------\n",
      "**Fencing response**\n",
      "\n",
      "The fencing response is a peculiar position of the arms following a concussion. Immediately after moderate forces have been applied to the brainstem, the forearms are held flexed or extended (typically into the air) for a period lasting up to several seconds after the impact. The fencing response is often observed during athletic competition involving contact, such as American football, hockey, rugby and martial arts. It is used as an overt indicator of injury force magnitude and midbrain localization to aid in injury identification and classification for events including, but not limited to, on-field and/or bystander observations of sports-related head injuries.\n",
      "\n",
      "***\n",
      "\n",
      "^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&message=Excludeme&subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/nfl/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot)   ^]\n",
      "^Downvote ^to ^remove ^| ^v0.24\n"
     ]
    }
   ],
   "source": [
    "# Example of a well predicted comment of a non-fan\n",
    "print('-------------------------')\n",
    "print('----predicted as: {}'.format(model_preds.sort_values('prob_off')['predicted_class'][0]))\n",
    "print('----actually: {}---(1 = fan, 0 = not fan --------'.format(model_preds.sort_values('prob_off')['actual_class'][0]))\n",
    "print('-------comment-----------')\n",
    "print(model_preds.sort_values('prob_off').index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "----predicted as: 0.9008954763412476\n",
      "----actually: 1---(1 = fan, 0 = not fan --------\n",
      "-------comment-----------\n",
      "Our only starting OL is our LT. Our ILB medically retired, we don't have our 1st, 2nd, 3rd, WR, our three starting RBs and our breakout CB last year in Tavon Young. Our other CB Hill who showed out in preseason is hurt. Our 2nd and 3rd TE are out. We have 20 on IR. \n",
      "\n",
      "We are 3-3, tied 6-6 with Sota and one game out of 1st in our division with 2 division wins. \n"
     ]
    }
   ],
   "source": [
    "# Example of a well predicted comment of a fan\n",
    "\n",
    "\n",
    "\n",
    "print('-------------------------')\n",
    "print('----predicted as: {}'.format(model_preds.loc[model_preds['actual_class'] == 1].sort_values('prob_off')['predicted_class'][2]))\n",
    "print('----actually: {}---(1 = fan, 0 = not fan --------'.format(model_preds.loc[model_preds['actual_class'] == 1].sort_values('prob_off')['actual_class'][2]))\n",
    "print('-------comment-----------')\n",
    "print(model_preds.loc[model_preds['actual_class'] == 1].sort_values('prob_off').index[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "----predicted as: 0.0809553861618042\n",
      "----actually: 1---(1 = fan, 0 = not fan --------\n",
      "-------comment-----------\n",
      "What...what do you mean?  \n",
      "\n",
      "Edit:  I'll attempt to answer.\n",
      "\n",
      "> *How* is New England the 1 seed in the AFC?\n",
      "\n",
      "New England is the 1 seed because they beat the Pittsburgh Steelers to take first seed, due to tiebreaker going to the Pats due to the win.  \n",
      "\n",
      "> How is *New England* the 1 seed in the AFC?\n",
      "\n",
      "They went 4-2 in their first 6 games of the season, arguably the hardest games on the schedule.  They finish off the season with mostly divisional matchups, for which they are currently 4-1 in.  \n",
      "\n",
      "> How is New England the *1* seed in the AFC?\n",
      "\n",
      "Tiebreaker over Pittsburgh.  \n",
      "\n",
      "> How is New England the 1 seed in the *AFC*?\n",
      "\n",
      "The Patriots play in the AFC.  \n"
     ]
    }
   ],
   "source": [
    "# Example of a poorly predicted comment of a non-fan\n",
    "print('-------------------------')\n",
    "print('----predicted as: {}'.format(model_preds.sort_values('prob_off')['predicted_class'][-1]))\n",
    "print('----actually: {}---(1 = fan, 0 = not fan --------'.format(model_preds.sort_values('prob_off')['actual_class'][-1]))\n",
    "print('-------comment-----------')\n",
    "print(model_preds.sort_values('prob_off').index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "----predicted as: 0.0934571623802185\n",
      "----actually: 1---(1 = fan, 0 = not fan --------\n",
      "-------comment-----------\n",
      "In the lead-up to the eclipse, I read a lot about the damage you could do to your eyes by looking at it without the proper protective glasses on. They said optomotrists would be able to tell you looked at the sun, because the outline of the eclipse would be burned into the cornea. For the rest of your life, that image is literally burned into your eye. It never goes away. It never heals. The damage is permanent.\n",
      "\n",
      "If you look into my eyes, you can see the outline of the David Tyree helmet catch. It never goes away. It never heals. The damage is permanent.\n",
      "\n",
      "I wish you the best this season, my friend.\n"
     ]
    }
   ],
   "source": [
    "# Example of a poorly predicted comment of a fan\n",
    "print('-------------------------')\n",
    "print('----predicted as: {}'.format(model_preds.sort_values('prob_off')['predicted_class'][-10]))\n",
    "print('----actually: {}---(1 = fan, 0 = not fan --------'.format(model_preds.sort_values('prob_off')['actual_class'][-10]))\n",
    "print('-------comment-----------')\n",
    "print(model_preds.sort_values('prob_off').index[-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "----predicted as: 0.8980789184570312\n",
      "----actually: 0---(1 = fan, 0 = not fan --------\n",
      "-------comment-----------\n",
      "the biggest problem is injuries and in a close second our pass blocking. if we were healthy and our pass blocking was competent our pass heavy offense would work but eli has had 2 seconds to throw the ball and has made bad throws with players wide open. you can definitely say our defense has still been good regardless of the shit show our offense has been and that is without our best coverage LB and last game without our MLB and Jackrabbit.  \n"
     ]
    }
   ],
   "source": [
    "# Example of a poorly predicted comment of a non-fan\n",
    "# They actually sound like a fan - probably mislabeled\n",
    "print('-------------------------')\n",
    "print('----predicted as: {}'.format(model_preds.sort_values('prob_off')['predicted_class'][-21]))\n",
    "print('----actually: {}---(1 = fan, 0 = not fan --------'.format(model_preds.sort_values('prob_off')['actual_class'][-21]))\n",
    "print('-------comment-----------')\n",
    "print(model_preds.sort_values('prob_off').index[-21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "----predicted as: 0.8941841125488281\n",
      "----actually: 0---(1 = fan, 0 = not fan --------\n",
      "-------comment-----------\n",
      "There defense has only allowed one team to score 20 points and in the 4th quarter the offense tends to get their head out of their asses. Our offense is 32nd in the league and the defense has the ability to be top 10. Our oline is killing us. Our LT is hurt and we are missing both of our starting guards. It's bad\n"
     ]
    }
   ],
   "source": [
    "# Example of a poorly predicted comment of a non-fan\n",
    "# They actually sound like a fan too! Probably mislabeled\n",
    "print('-------------------------')\n",
    "print('----predicted as: {}'.format(model_preds.sort_values('prob_off')['predicted_class'][-29]))\n",
    "print('----actually: {}---(1 = fan, 0 = not fan --------'.format(model_preds.sort_values('prob_off')['actual_class'][-29]))\n",
    "print('-------comment-----------')\n",
    "print(model_preds.sort_values('prob_off').index[-29])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the model to predict some test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = ['we are awesome',\n",
    "         'our team is awesome',\n",
    "         'jags suck',\n",
    "         'when will we win',\n",
    "         'we are so dumb',\n",
    "         'they suck',\n",
    "         'why arent we winning',\n",
    "        'this is hilarious',\n",
    "        'just tuned in',\n",
    "        'rigged refs',\n",
    "        'i hate football',\n",
    "        'i love football',\n",
    "        'that was a horrible call',\n",
    "        'we are so good',\n",
    "        'they',\n",
    "        'we',\n",
    "        'tom brady is so good that they dont know what to do with him',\n",
    "        'I love Lions fans cause we can drink bleach together this christmas eve']\n",
    "encoded_test = [one_hot(str(d), vocab_size) for d in tests]\n",
    "padded_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
    "preds_test = model.predict(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>we are awesome</th>\n",
       "      <td>0.647458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>our team is awesome</th>\n",
       "      <td>0.767350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jags suck</th>\n",
       "      <td>0.232012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>when will we win</th>\n",
       "      <td>0.749833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we are so dumb</th>\n",
       "      <td>0.714126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they suck</th>\n",
       "      <td>0.451200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>why arent we winning</th>\n",
       "      <td>0.752060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this is hilarious</th>\n",
       "      <td>0.378172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just tuned in</th>\n",
       "      <td>0.209693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rigged refs</th>\n",
       "      <td>0.468423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i hate football</th>\n",
       "      <td>0.424831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i love football</th>\n",
       "      <td>0.424482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that was a horrible call</th>\n",
       "      <td>0.439760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we are so good</th>\n",
       "      <td>0.724073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>0.451200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>0.715281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tom brady is so good that they dont know what to do with him</th>\n",
       "      <td>0.317711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I love Lions fans cause we can drink bleach together this christmas eve</th>\n",
       "      <td>0.813379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    prediction\n",
       "we are awesome                                        0.647458\n",
       "our team is awesome                                   0.767350\n",
       "jags suck                                             0.232012\n",
       "when will we win                                      0.749833\n",
       "we are so dumb                                        0.714126\n",
       "they suck                                             0.451200\n",
       "why arent we winning                                  0.752060\n",
       "this is hilarious                                     0.378172\n",
       "just tuned in                                         0.209693\n",
       "rigged refs                                           0.468423\n",
       "i hate football                                       0.424831\n",
       "i love football                                       0.424482\n",
       "that was a horrible call                              0.439760\n",
       "we are so good                                        0.724073\n",
       "they                                                  0.451200\n",
       "we                                                    0.715281\n",
       "tom brady is so good that they dont know what t...    0.317711\n",
       "I love Lions fans cause we can drink bleach tog...    0.813379"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = pd.DataFrame(index=tests)\n",
    "test_results['prediction'] = 0\n",
    "test_results['prediction'] = preds_test\n",
    "# Predicted class is actually the inverse (I think)\n",
    "test_results['prediction'] = 1 - test_results['prediction']\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
