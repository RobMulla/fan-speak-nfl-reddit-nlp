{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from: https://github.com/dennybritz/cnn-text-classification-tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kwheatley/anaconda/envs/python36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7675\n",
      "Train/Dev split: 10295/1143\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "dev_sample_percentage = .1\n",
    "low_data_file = \"./data/rt-polaritydata/win.low\"\n",
    "medium_data_file = \"./data/rt-polaritydata/win.medium\"\n",
    "high_data_file = \"./data/rt-polaritydata/win.high\"\n",
    "\n",
    "\n",
    "x_text, y = data_helpers.load_data_and_labels(low_data_file, \n",
    "                                              medium_data_file,\n",
    "                                              high_data_file)\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give brady the jags o line and this game is so different zero time in the pocket this game so far\n",
      "[   7   11   34    3    4 2422  286   41   75  483   20 1376  382 2884\n",
      " 2029  594 1015  185    3  912   14    7  141   27 3958   23  892    7\n",
      "  212   47  737   41 2404    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Spot checking word ids creation\n",
    "print(x_text[0])\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10295, 142)\n",
      "(10295, 3)\n",
      "(10295,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "y_train_nb = y_train[:,2]*2+y_train[:,1]\n",
    "y_dev_nb = y_dev[:,2]*2+y_dev[:,1]\n",
    "print(y_train_nb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 37.27%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "nb = MultinomialNB()\n",
    "nb.fit(x_train, y_train_nb)\n",
    "y_pred = nb.predict(x_dev)\n",
    "\n",
    "acc = accuracy_score(y_dev_nb, y_pred)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Common Class Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3682370082564352\n",
      "0.36132983377077865\n"
     ]
    }
   ],
   "source": [
    "# Most common percent\n",
    "from collections import Counter\n",
    "list_cnts = Counter(y_train_nb)\n",
    "highest = sorted(list_cnts, key = list_cnts.get, reverse = True)[:1]\n",
    "highest_percent = list_cnts[highest[0]]/len(y_train_nb)\n",
    "print(highest_percent)\n",
    "\n",
    "# Accuracy for test data\n",
    "list_cnts = Counter(y_dev_nb)\n",
    "highest = sorted(list_cnts, key = list_cnts.get, reverse = True)[:1]\n",
    "highest_percent = list_cnts[highest[0]]/len(y_dev)\n",
    "print(highest_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 128\n",
    "filter_sizes = \"3,4,5\"\n",
    "num_filters = 128\n",
    "dropout_keep_prob = 0.5\n",
    "l2_reg_lambda = 0.0\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64 \n",
    "num_epochs = 5 #200\n",
    "evaluate_every = 100\n",
    "checkpoint_every = 100\n",
    "num_checkpoints = 5\n",
    "\n",
    "# Misc Parameters\n",
    "allow_soft_placement = True\n",
    "log_device_placement = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kwheatley/Desktop/w266_nfl/cnn-text-classification-tf/text_cnn.py:78: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/kwheatley/Desktop/w266_nfl/cnn-text-classification-tf/runs/1521461861\n",
      "\n",
      "2018-03-19T07:17:44.300126: step 1, loss 2.50132, acc 0.46875\n",
      "2018-03-19T07:17:44.765311: step 2, loss 2.59743, acc 0.375\n",
      "2018-03-19T07:17:45.237331: step 3, loss 2.91528, acc 0.390625\n",
      "2018-03-19T07:17:45.681652: step 4, loss 2.55683, acc 0.421875\n",
      "2018-03-19T07:17:46.361034: step 5, loss 2.77847, acc 0.34375\n",
      "2018-03-19T07:17:46.860252: step 6, loss 2.37251, acc 0.359375\n",
      "2018-03-19T07:17:47.469822: step 7, loss 2.80293, acc 0.296875\n",
      "2018-03-19T07:17:48.080714: step 8, loss 2.69921, acc 0.359375\n",
      "2018-03-19T07:17:48.578201: step 9, loss 3.33375, acc 0.21875\n",
      "2018-03-19T07:17:49.145777: step 10, loss 2.54291, acc 0.34375\n",
      "2018-03-19T07:17:49.752658: step 11, loss 2.12928, acc 0.4375\n",
      "2018-03-19T07:17:50.274144: step 12, loss 2.5558, acc 0.234375\n",
      "2018-03-19T07:17:50.805898: step 13, loss 2.04207, acc 0.453125\n",
      "2018-03-19T07:17:51.315676: step 14, loss 2.45795, acc 0.40625\n",
      "2018-03-19T07:17:51.774116: step 15, loss 2.99118, acc 0.265625\n",
      "2018-03-19T07:17:52.246579: step 16, loss 2.17553, acc 0.421875\n",
      "2018-03-19T07:17:52.710288: step 17, loss 2.61704, acc 0.265625\n",
      "2018-03-19T07:17:53.170474: step 18, loss 2.53209, acc 0.359375\n",
      "2018-03-19T07:17:53.692361: step 19, loss 2.42173, acc 0.375\n",
      "2018-03-19T07:17:54.151398: step 20, loss 1.94085, acc 0.328125\n",
      "2018-03-19T07:17:54.608029: step 21, loss 2.13544, acc 0.4375\n",
      "2018-03-19T07:17:55.061669: step 22, loss 2.26528, acc 0.390625\n",
      "2018-03-19T07:17:55.500586: step 23, loss 1.80502, acc 0.359375\n",
      "2018-03-19T07:17:56.090887: step 24, loss 2.67594, acc 0.3125\n",
      "2018-03-19T07:17:56.686558: step 25, loss 2.08061, acc 0.296875\n",
      "2018-03-19T07:17:57.146045: step 26, loss 1.81199, acc 0.375\n",
      "2018-03-19T07:17:57.679824: step 27, loss 2.14727, acc 0.453125\n",
      "2018-03-19T07:17:58.121145: step 28, loss 2.41405, acc 0.296875\n",
      "2018-03-19T07:17:58.639946: step 29, loss 1.98845, acc 0.453125\n",
      "2018-03-19T07:17:59.102282: step 30, loss 2.03532, acc 0.34375\n",
      "2018-03-19T07:17:59.557813: step 31, loss 2.21307, acc 0.375\n",
      "2018-03-19T07:18:00.002905: step 32, loss 2.36624, acc 0.359375\n",
      "2018-03-19T07:18:00.443439: step 33, loss 2.01929, acc 0.390625\n",
      "2018-03-19T07:18:00.913801: step 34, loss 2.61837, acc 0.359375\n",
      "2018-03-19T07:18:01.370544: step 35, loss 2.35102, acc 0.328125\n",
      "2018-03-19T07:18:01.838950: step 36, loss 2.71737, acc 0.3125\n",
      "2018-03-19T07:18:02.410456: step 37, loss 1.73842, acc 0.40625\n",
      "2018-03-19T07:18:03.143377: step 38, loss 2.43103, acc 0.3125\n",
      "2018-03-19T07:18:03.769275: step 39, loss 2.34111, acc 0.34375\n",
      "2018-03-19T07:18:04.328024: step 40, loss 2.13437, acc 0.421875\n",
      "2018-03-19T07:18:04.875524: step 41, loss 2.48481, acc 0.28125\n",
      "2018-03-19T07:18:05.374538: step 42, loss 2.07722, acc 0.359375\n",
      "2018-03-19T07:18:05.883886: step 43, loss 2.09684, acc 0.3125\n",
      "2018-03-19T07:18:06.405789: step 44, loss 2.10366, acc 0.359375\n",
      "2018-03-19T07:18:06.926235: step 45, loss 2.95862, acc 0.296875\n",
      "2018-03-19T07:18:07.421136: step 46, loss 2.14697, acc 0.328125\n",
      "2018-03-19T07:18:07.865958: step 47, loss 1.92017, acc 0.390625\n",
      "2018-03-19T07:18:08.394205: step 48, loss 2.05509, acc 0.4375\n",
      "2018-03-19T07:18:08.958172: step 49, loss 1.72795, acc 0.34375\n",
      "2018-03-19T07:18:09.415392: step 50, loss 2.258, acc 0.28125\n",
      "2018-03-19T07:18:09.928234: step 51, loss 2.01835, acc 0.328125\n",
      "2018-03-19T07:18:10.379202: step 52, loss 1.59866, acc 0.40625\n",
      "2018-03-19T07:18:10.827810: step 53, loss 2.1373, acc 0.375\n",
      "2018-03-19T07:18:11.554559: step 54, loss 2.00731, acc 0.4375\n",
      "2018-03-19T07:18:12.126399: step 55, loss 1.73779, acc 0.46875\n",
      "2018-03-19T07:18:12.597801: step 56, loss 2.33157, acc 0.296875\n",
      "2018-03-19T07:18:13.119305: step 57, loss 2.2797, acc 0.375\n",
      "2018-03-19T07:18:13.701054: step 58, loss 2.42772, acc 0.328125\n",
      "2018-03-19T07:18:14.394256: step 59, loss 2.06488, acc 0.4375\n",
      "2018-03-19T07:18:14.891554: step 60, loss 2.13966, acc 0.375\n",
      "2018-03-19T07:18:15.471525: step 61, loss 2.14208, acc 0.34375\n",
      "2018-03-19T07:18:15.932160: step 62, loss 2.04974, acc 0.3125\n",
      "2018-03-19T07:18:16.519270: step 63, loss 1.84783, acc 0.359375\n",
      "2018-03-19T07:18:17.246974: step 64, loss 2.52549, acc 0.296875\n",
      "2018-03-19T07:18:17.736124: step 65, loss 1.89847, acc 0.4375\n",
      "2018-03-19T07:18:18.271745: step 66, loss 1.71035, acc 0.328125\n",
      "2018-03-19T07:18:18.839531: step 67, loss 1.77192, acc 0.4375\n",
      "2018-03-19T07:18:19.343408: step 68, loss 2.07434, acc 0.390625\n",
      "2018-03-19T07:18:19.780453: step 69, loss 1.74106, acc 0.40625\n",
      "2018-03-19T07:18:20.406841: step 70, loss 1.88262, acc 0.34375\n",
      "2018-03-19T07:18:20.920728: step 71, loss 1.9341, acc 0.28125\n",
      "2018-03-19T07:18:21.427782: step 72, loss 1.79298, acc 0.375\n",
      "2018-03-19T07:18:21.899659: step 73, loss 1.92834, acc 0.4375\n",
      "2018-03-19T07:18:22.421214: step 74, loss 2.13922, acc 0.40625\n",
      "2018-03-19T07:18:22.947472: step 75, loss 1.92309, acc 0.265625\n",
      "2018-03-19T07:18:23.434809: step 76, loss 2.06419, acc 0.328125\n",
      "2018-03-19T07:18:23.891073: step 77, loss 2.14009, acc 0.359375\n",
      "2018-03-19T07:18:24.390226: step 78, loss 2.06306, acc 0.265625\n",
      "2018-03-19T07:18:24.926596: step 79, loss 1.74856, acc 0.34375\n",
      "2018-03-19T07:18:25.459770: step 80, loss 1.48884, acc 0.4375\n",
      "2018-03-19T07:18:25.955793: step 81, loss 1.85311, acc 0.3125\n",
      "2018-03-19T07:18:26.416361: step 82, loss 1.53706, acc 0.453125\n",
      "2018-03-19T07:18:26.939373: step 83, loss 2.1245, acc 0.375\n",
      "2018-03-19T07:18:27.431277: step 84, loss 1.50921, acc 0.546875\n",
      "2018-03-19T07:18:27.898350: step 85, loss 1.78887, acc 0.390625\n",
      "2018-03-19T07:18:28.371168: step 86, loss 2.06788, acc 0.3125\n",
      "2018-03-19T07:18:28.841413: step 87, loss 2.31357, acc 0.265625\n",
      "2018-03-19T07:18:29.359096: step 88, loss 1.82372, acc 0.3125\n",
      "2018-03-19T07:18:29.899022: step 89, loss 1.91326, acc 0.375\n",
      "2018-03-19T07:18:30.400135: step 90, loss 2.06581, acc 0.296875\n",
      "2018-03-19T07:18:30.893707: step 91, loss 1.4173, acc 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-19T07:18:31.388576: step 92, loss 2.19109, acc 0.265625\n",
      "2018-03-19T07:18:31.908727: step 93, loss 1.69365, acc 0.296875\n",
      "2018-03-19T07:18:32.369183: step 94, loss 2.50262, acc 0.28125\n",
      "2018-03-19T07:18:32.865938: step 95, loss 2.12053, acc 0.265625\n",
      "2018-03-19T07:18:33.339869: step 96, loss 1.84941, acc 0.453125\n",
      "2018-03-19T07:18:33.833016: step 97, loss 1.94382, acc 0.328125\n",
      "2018-03-19T07:18:34.316673: step 98, loss 1.61944, acc 0.421875\n",
      "2018-03-19T07:18:34.790082: step 99, loss 2.00684, acc 0.375\n",
      "2018-03-19T07:18:35.240064: step 100, loss 2.02141, acc 0.40625\n",
      "\n",
      "Evaluation:\n",
      "2018-03-19T07:18:37.874239: step 100, loss 1.28262, acc 0.381452\n",
      "\n",
      "Saved model checkpoint to /Users/kwheatley/Desktop/w266_nfl/cnn-text-classification-tf/runs/1521461861/checkpoints/model-100\n",
      "\n",
      "2018-03-19T07:18:39.028634: step 101, loss 1.31917, acc 0.5\n",
      "2018-03-19T07:18:39.531739: step 102, loss 1.73949, acc 0.359375\n",
      "2018-03-19T07:18:40.091731: step 103, loss 1.47552, acc 0.359375\n",
      "2018-03-19T07:18:40.550308: step 104, loss 2.10563, acc 0.359375\n",
      "2018-03-19T07:18:41.032928: step 105, loss 1.56879, acc 0.453125\n",
      "2018-03-19T07:18:41.505678: step 106, loss 2.2553, acc 0.296875\n",
      "2018-03-19T07:18:41.988362: step 107, loss 1.48843, acc 0.375\n",
      "2018-03-19T07:18:42.466346: step 108, loss 1.92428, acc 0.359375\n",
      "2018-03-19T07:18:42.929140: step 109, loss 1.77732, acc 0.46875\n",
      "2018-03-19T07:18:43.448226: step 110, loss 1.53507, acc 0.390625\n",
      "2018-03-19T07:18:43.924062: step 111, loss 1.40677, acc 0.375\n",
      "2018-03-19T07:18:44.386051: step 112, loss 1.85629, acc 0.421875\n",
      "2018-03-19T07:18:44.844064: step 113, loss 1.72461, acc 0.34375\n",
      "2018-03-19T07:18:45.335126: step 114, loss 1.9381, acc 0.265625\n",
      "2018-03-19T07:18:45.784247: step 115, loss 1.78824, acc 0.359375\n",
      "2018-03-19T07:18:46.232989: step 116, loss 1.62963, acc 0.421875\n",
      "2018-03-19T07:18:46.934842: step 117, loss 2.09552, acc 0.359375\n",
      "2018-03-19T07:18:47.518574: step 118, loss 1.8458, acc 0.375\n",
      "2018-03-19T07:18:48.063234: step 119, loss 1.7349, acc 0.3125\n",
      "2018-03-19T07:18:48.549003: step 120, loss 1.82032, acc 0.359375\n",
      "2018-03-19T07:18:49.069127: step 121, loss 1.74909, acc 0.40625\n",
      "2018-03-19T07:18:49.716046: step 122, loss 2.11199, acc 0.3125\n",
      "2018-03-19T07:18:50.347466: step 123, loss 1.89257, acc 0.296875\n",
      "2018-03-19T07:18:50.841915: step 124, loss 2.00744, acc 0.34375\n",
      "2018-03-19T07:18:51.349266: step 125, loss 1.99122, acc 0.34375\n",
      "2018-03-19T07:18:51.847510: step 126, loss 2.08936, acc 0.34375\n",
      "2018-03-19T07:18:52.356260: step 127, loss 1.99566, acc 0.3125\n",
      "2018-03-19T07:18:52.913005: step 128, loss 1.86364, acc 0.34375\n",
      "2018-03-19T07:18:53.555304: step 129, loss 1.51961, acc 0.34375\n",
      "2018-03-19T07:18:54.116502: step 130, loss 1.57687, acc 0.296875\n",
      "2018-03-19T07:18:54.635021: step 131, loss 1.91264, acc 0.46875\n",
      "2018-03-19T07:18:55.159129: step 132, loss 1.63586, acc 0.34375\n",
      "2018-03-19T07:18:55.616878: step 133, loss 1.73403, acc 0.375\n",
      "2018-03-19T07:18:56.068527: step 134, loss 2.21795, acc 0.28125\n",
      "2018-03-19T07:18:56.525339: step 135, loss 1.62831, acc 0.375\n",
      "2018-03-19T07:18:57.032961: step 136, loss 1.70382, acc 0.359375\n",
      "2018-03-19T07:18:57.558529: step 137, loss 1.62256, acc 0.34375\n",
      "2018-03-19T07:18:58.110153: step 138, loss 1.59106, acc 0.34375\n",
      "2018-03-19T07:18:58.608702: step 139, loss 2.10415, acc 0.265625\n",
      "2018-03-19T07:18:59.108317: step 140, loss 1.70983, acc 0.390625\n",
      "2018-03-19T07:18:59.659176: step 141, loss 1.91754, acc 0.34375\n",
      "2018-03-19T07:19:00.132933: step 142, loss 1.539, acc 0.4375\n",
      "2018-03-19T07:19:00.593863: step 143, loss 1.46497, acc 0.46875\n",
      "2018-03-19T07:19:01.133865: step 144, loss 1.82367, acc 0.296875\n",
      "2018-03-19T07:19:01.607454: step 145, loss 1.9262, acc 0.296875\n",
      "2018-03-19T07:19:02.071665: step 146, loss 1.60723, acc 0.359375\n",
      "2018-03-19T07:19:02.536363: step 147, loss 1.88189, acc 0.421875\n",
      "2018-03-19T07:19:03.006517: step 148, loss 1.55894, acc 0.390625\n",
      "2018-03-19T07:19:03.470272: step 149, loss 1.78366, acc 0.390625\n",
      "2018-03-19T07:19:03.912734: step 150, loss 1.26053, acc 0.46875\n",
      "2018-03-19T07:19:04.409193: step 151, loss 1.37783, acc 0.46875\n",
      "2018-03-19T07:19:04.898312: step 152, loss 1.57648, acc 0.40625\n",
      "2018-03-19T07:19:05.351838: step 153, loss 1.70274, acc 0.265625\n",
      "2018-03-19T07:19:05.828768: step 154, loss 1.6826, acc 0.296875\n",
      "2018-03-19T07:19:06.277641: step 155, loss 1.3843, acc 0.421875\n",
      "2018-03-19T07:19:06.743710: step 156, loss 1.44609, acc 0.359375\n",
      "2018-03-19T07:19:07.239453: step 157, loss 1.69441, acc 0.296875\n",
      "2018-03-19T07:19:07.792110: step 158, loss 1.50036, acc 0.375\n",
      "2018-03-19T07:19:08.383007: step 159, loss 1.52697, acc 0.375\n",
      "2018-03-19T07:19:08.850592: step 160, loss 1.74793, acc 0.390625\n",
      "2018-03-19T07:19:09.263535: step 161, loss 1.62141, acc 0.418182\n",
      "2018-03-19T07:19:09.764116: step 162, loss 1.83385, acc 0.40625\n",
      "2018-03-19T07:19:10.249281: step 163, loss 1.23722, acc 0.484375\n",
      "2018-03-19T07:19:10.717304: step 164, loss 1.62229, acc 0.296875\n",
      "2018-03-19T07:19:11.228091: step 165, loss 1.55558, acc 0.375\n",
      "2018-03-19T07:19:11.698134: step 166, loss 1.31233, acc 0.484375\n",
      "2018-03-19T07:19:12.222063: step 167, loss 1.67197, acc 0.359375\n",
      "2018-03-19T07:19:12.713836: step 168, loss 1.16222, acc 0.546875\n",
      "2018-03-19T07:19:13.212492: step 169, loss 1.49179, acc 0.421875\n",
      "2018-03-19T07:19:13.683945: step 170, loss 1.49427, acc 0.390625\n",
      "2018-03-19T07:19:14.184100: step 171, loss 1.23056, acc 0.453125\n",
      "2018-03-19T07:19:14.656689: step 172, loss 1.30572, acc 0.421875\n",
      "2018-03-19T07:19:15.156067: step 173, loss 1.48995, acc 0.484375\n",
      "2018-03-19T07:19:15.613994: step 174, loss 1.57571, acc 0.359375\n",
      "2018-03-19T07:19:16.082774: step 175, loss 1.54189, acc 0.4375\n",
      "2018-03-19T07:19:16.534328: step 176, loss 1.29475, acc 0.4375\n",
      "2018-03-19T07:19:16.999143: step 177, loss 1.56504, acc 0.328125\n",
      "2018-03-19T07:19:17.459377: step 178, loss 1.49261, acc 0.421875\n",
      "2018-03-19T07:19:17.933918: step 179, loss 1.44044, acc 0.3125\n",
      "2018-03-19T07:19:18.388403: step 180, loss 1.29346, acc 0.421875\n",
      "2018-03-19T07:19:18.833371: step 181, loss 1.22685, acc 0.40625\n",
      "2018-03-19T07:19:19.280697: step 182, loss 1.44435, acc 0.4375\n",
      "2018-03-19T07:19:19.733266: step 183, loss 1.62929, acc 0.328125\n",
      "2018-03-19T07:19:20.232561: step 184, loss 1.40083, acc 0.46875\n",
      "2018-03-19T07:19:20.781970: step 185, loss 1.20818, acc 0.4375\n",
      "2018-03-19T07:19:21.381711: step 186, loss 1.71554, acc 0.28125\n",
      "2018-03-19T07:19:21.933183: step 187, loss 1.51829, acc 0.40625\n",
      "2018-03-19T07:19:22.439215: step 188, loss 1.32543, acc 0.453125\n",
      "2018-03-19T07:19:23.041818: step 189, loss 1.48151, acc 0.484375\n",
      "2018-03-19T07:19:23.487101: step 190, loss 1.57917, acc 0.3125\n",
      "2018-03-19T07:19:23.964360: step 191, loss 1.48435, acc 0.421875\n",
      "2018-03-19T07:19:24.427174: step 192, loss 1.38542, acc 0.421875\n",
      "2018-03-19T07:19:24.887493: step 193, loss 1.29703, acc 0.46875\n",
      "2018-03-19T07:19:25.366040: step 194, loss 1.51388, acc 0.296875\n",
      "2018-03-19T07:19:25.832531: step 195, loss 1.46375, acc 0.390625\n",
      "2018-03-19T07:19:26.313682: step 196, loss 1.20504, acc 0.578125\n",
      "2018-03-19T07:19:26.772211: step 197, loss 1.476, acc 0.484375\n",
      "2018-03-19T07:19:27.238764: step 198, loss 1.66374, acc 0.484375\n",
      "2018-03-19T07:19:27.702471: step 199, loss 1.51545, acc 0.34375\n",
      "2018-03-19T07:19:28.172547: step 200, loss 1.3133, acc 0.40625\n",
      "\n",
      "Evaluation:\n",
      "2018-03-19T07:19:30.572176: step 200, loss 1.1654, acc 0.396325\n",
      "\n",
      "Saved model checkpoint to /Users/kwheatley/Desktop/w266_nfl/cnn-text-classification-tf/runs/1521461861/checkpoints/model-200\n",
      "\n",
      "2018-03-19T07:19:31.573760: step 201, loss 1.26807, acc 0.453125\n",
      "2018-03-19T07:19:32.158667: step 202, loss 1.29824, acc 0.515625\n",
      "2018-03-19T07:19:32.616729: step 203, loss 1.27253, acc 0.40625\n",
      "2018-03-19T07:19:33.165537: step 204, loss 1.42692, acc 0.421875\n",
      "2018-03-19T07:19:33.614791: step 205, loss 1.37426, acc 0.46875\n",
      "2018-03-19T07:19:34.072030: step 206, loss 1.37877, acc 0.421875\n",
      "2018-03-19T07:19:34.512597: step 207, loss 0.991606, acc 0.625\n",
      "2018-03-19T07:19:34.982087: step 208, loss 1.42917, acc 0.359375\n",
      "2018-03-19T07:19:35.439189: step 209, loss 1.69519, acc 0.3125\n",
      "2018-03-19T07:19:35.893949: step 210, loss 1.27581, acc 0.421875\n",
      "2018-03-19T07:19:36.341908: step 211, loss 1.23103, acc 0.421875\n",
      "2018-03-19T07:19:36.787689: step 212, loss 1.53704, acc 0.359375\n",
      "2018-03-19T07:19:37.242008: step 213, loss 1.51589, acc 0.359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-19T07:19:37.722539: step 214, loss 1.25005, acc 0.484375\n",
      "2018-03-19T07:19:38.202815: step 215, loss 1.53457, acc 0.375\n",
      "2018-03-19T07:19:38.666247: step 216, loss 1.43069, acc 0.328125\n",
      "2018-03-19T07:19:39.119472: step 217, loss 1.37477, acc 0.484375\n",
      "2018-03-19T07:19:39.560719: step 218, loss 1.475, acc 0.40625\n",
      "2018-03-19T07:19:40.034343: step 219, loss 1.26043, acc 0.375\n",
      "2018-03-19T07:19:40.491222: step 220, loss 1.38957, acc 0.421875\n",
      "2018-03-19T07:19:40.972694: step 221, loss 1.37557, acc 0.421875\n",
      "2018-03-19T07:19:41.410185: step 222, loss 1.50845, acc 0.390625\n",
      "2018-03-19T07:19:41.859106: step 223, loss 1.39171, acc 0.40625\n",
      "2018-03-19T07:19:42.321211: step 224, loss 1.52559, acc 0.328125\n",
      "2018-03-19T07:19:42.763103: step 225, loss 1.30942, acc 0.390625\n",
      "2018-03-19T07:19:43.209146: step 226, loss 1.28004, acc 0.53125\n",
      "2018-03-19T07:19:43.650395: step 227, loss 1.46537, acc 0.46875\n",
      "2018-03-19T07:19:44.135255: step 228, loss 1.74854, acc 0.3125\n",
      "2018-03-19T07:19:44.610263: step 229, loss 1.45632, acc 0.390625\n",
      "2018-03-19T07:19:45.128682: step 230, loss 1.15039, acc 0.4375\n",
      "2018-03-19T07:19:45.579060: step 231, loss 1.41692, acc 0.484375\n",
      "2018-03-19T07:19:46.032755: step 232, loss 1.59461, acc 0.34375\n",
      "2018-03-19T07:19:46.539736: step 233, loss 1.15602, acc 0.4375\n",
      "2018-03-19T07:19:47.008747: step 234, loss 1.30565, acc 0.390625\n",
      "2018-03-19T07:19:47.486248: step 235, loss 1.43449, acc 0.4375\n",
      "2018-03-19T07:19:47.948872: step 236, loss 1.57466, acc 0.3125\n",
      "2018-03-19T07:19:48.385768: step 237, loss 1.53247, acc 0.328125\n",
      "2018-03-19T07:19:48.817221: step 238, loss 1.3419, acc 0.46875\n",
      "2018-03-19T07:19:49.255845: step 239, loss 1.31677, acc 0.40625\n",
      "2018-03-19T07:19:49.692559: step 240, loss 1.4149, acc 0.453125\n",
      "2018-03-19T07:19:50.127342: step 241, loss 1.66989, acc 0.296875\n",
      "2018-03-19T07:19:50.562284: step 242, loss 1.05354, acc 0.609375\n",
      "2018-03-19T07:19:51.046099: step 243, loss 1.28636, acc 0.453125\n",
      "2018-03-19T07:19:51.514734: step 244, loss 1.29304, acc 0.5\n",
      "2018-03-19T07:19:52.001696: step 245, loss 1.32617, acc 0.40625\n",
      "2018-03-19T07:19:52.497842: step 246, loss 1.48501, acc 0.4375\n",
      "2018-03-19T07:19:53.070770: step 247, loss 1.27362, acc 0.453125\n",
      "2018-03-19T07:19:53.606588: step 248, loss 1.54814, acc 0.34375\n",
      "2018-03-19T07:19:54.100421: step 249, loss 1.27457, acc 0.421875\n",
      "2018-03-19T07:19:54.555627: step 250, loss 1.15104, acc 0.421875\n",
      "2018-03-19T07:19:55.051182: step 251, loss 1.20437, acc 0.34375\n",
      "2018-03-19T07:19:55.545435: step 252, loss 1.38153, acc 0.34375\n",
      "2018-03-19T07:19:56.045967: step 253, loss 1.41808, acc 0.28125\n",
      "2018-03-19T07:19:56.518197: step 254, loss 1.53407, acc 0.390625\n",
      "2018-03-19T07:19:56.986167: step 255, loss 1.24239, acc 0.421875\n",
      "2018-03-19T07:19:57.472313: step 256, loss 1.20327, acc 0.453125\n",
      "2018-03-19T07:19:57.979223: step 257, loss 1.36641, acc 0.390625\n",
      "2018-03-19T07:19:58.431676: step 258, loss 1.25294, acc 0.4375\n",
      "2018-03-19T07:19:58.869984: step 259, loss 1.25244, acc 0.46875\n",
      "2018-03-19T07:19:59.310970: step 260, loss 1.46757, acc 0.390625\n",
      "2018-03-19T07:19:59.744817: step 261, loss 1.38473, acc 0.453125\n",
      "2018-03-19T07:20:00.189449: step 262, loss 1.32849, acc 0.40625\n",
      "2018-03-19T07:20:00.619294: step 263, loss 1.20997, acc 0.46875\n",
      "2018-03-19T07:20:01.064375: step 264, loss 1.16056, acc 0.421875\n",
      "2018-03-19T07:20:01.501437: step 265, loss 1.28382, acc 0.453125\n",
      "2018-03-19T07:20:01.955737: step 266, loss 1.13737, acc 0.375\n",
      "2018-03-19T07:20:02.418949: step 267, loss 1.17379, acc 0.390625\n",
      "2018-03-19T07:20:02.897407: step 268, loss 1.27423, acc 0.484375\n",
      "2018-03-19T07:20:03.391381: step 269, loss 1.36869, acc 0.375\n",
      "2018-03-19T07:20:03.842836: step 270, loss 1.34738, acc 0.328125\n",
      "2018-03-19T07:20:04.310614: step 271, loss 1.33613, acc 0.375\n",
      "2018-03-19T07:20:04.779719: step 272, loss 1.38774, acc 0.421875\n",
      "2018-03-19T07:20:05.262427: step 273, loss 1.20557, acc 0.484375\n",
      "2018-03-19T07:20:05.706341: step 274, loss 1.27189, acc 0.453125\n",
      "2018-03-19T07:20:06.186577: step 275, loss 1.16959, acc 0.53125\n",
      "2018-03-19T07:20:06.637598: step 276, loss 1.16696, acc 0.46875\n",
      "2018-03-19T07:20:07.109345: step 277, loss 1.29991, acc 0.390625\n",
      "2018-03-19T07:20:07.671584: step 278, loss 1.35937, acc 0.40625\n",
      "2018-03-19T07:20:08.118097: step 279, loss 1.21352, acc 0.421875\n",
      "2018-03-19T07:20:08.630343: step 280, loss 1.35569, acc 0.375\n",
      "2018-03-19T07:20:09.329941: step 281, loss 1.37921, acc 0.421875\n",
      "2018-03-19T07:20:09.996389: step 282, loss 1.27657, acc 0.421875\n",
      "2018-03-19T07:20:10.563550: step 283, loss 1.31532, acc 0.484375\n",
      "2018-03-19T07:20:11.157237: step 284, loss 1.65208, acc 0.265625\n",
      "2018-03-19T07:20:11.777075: step 285, loss 1.24933, acc 0.453125\n",
      "2018-03-19T07:20:12.415208: step 286, loss 1.13964, acc 0.5\n",
      "2018-03-19T07:20:13.011541: step 287, loss 1.55058, acc 0.34375\n",
      "2018-03-19T07:20:13.541079: step 288, loss 1.26124, acc 0.4375\n",
      "2018-03-19T07:20:14.009766: step 289, loss 1.15932, acc 0.5\n",
      "2018-03-19T07:20:14.450281: step 290, loss 1.36895, acc 0.359375\n",
      "2018-03-19T07:20:15.009572: step 291, loss 1.33434, acc 0.3125\n",
      "2018-03-19T07:20:15.669382: step 292, loss 1.407, acc 0.390625\n",
      "2018-03-19T07:20:16.208728: step 293, loss 1.24113, acc 0.5\n",
      "2018-03-19T07:20:16.766724: step 294, loss 1.08266, acc 0.453125\n",
      "2018-03-19T07:20:17.366600: step 295, loss 1.19469, acc 0.515625\n",
      "2018-03-19T07:20:17.962168: step 296, loss 1.32777, acc 0.40625\n",
      "2018-03-19T07:20:18.461354: step 297, loss 1.48277, acc 0.296875\n",
      "2018-03-19T07:20:19.073042: step 298, loss 1.35808, acc 0.40625\n",
      "2018-03-19T07:20:19.672270: step 299, loss 1.29062, acc 0.453125\n",
      "2018-03-19T07:20:20.303400: step 300, loss 1.37137, acc 0.46875\n",
      "\n",
      "Evaluation:\n",
      "2018-03-19T07:20:23.630597: step 300, loss 1.1097, acc 0.405074\n",
      "\n",
      "Saved model checkpoint to /Users/kwheatley/Desktop/w266_nfl/cnn-text-classification-tf/runs/1521461861/checkpoints/model-300\n",
      "\n",
      "2018-03-19T07:20:24.999621: step 301, loss 1.28941, acc 0.484375\n",
      "2018-03-19T07:20:25.488203: step 302, loss 1.33371, acc 0.390625\n",
      "2018-03-19T07:20:26.012912: step 303, loss 1.25897, acc 0.40625\n",
      "2018-03-19T07:20:26.495369: step 304, loss 1.11311, acc 0.5\n",
      "2018-03-19T07:20:27.019016: step 305, loss 1.23987, acc 0.484375\n",
      "2018-03-19T07:20:27.492260: step 306, loss 1.18847, acc 0.5\n",
      "2018-03-19T07:20:28.018677: step 307, loss 1.25827, acc 0.453125\n",
      "2018-03-19T07:20:28.517554: step 308, loss 1.46298, acc 0.390625\n",
      "2018-03-19T07:20:28.975304: step 309, loss 1.24265, acc 0.453125\n",
      "2018-03-19T07:20:29.546919: step 310, loss 1.26503, acc 0.359375\n",
      "2018-03-19T07:20:30.017023: step 311, loss 1.07258, acc 0.46875\n",
      "2018-03-19T07:20:30.467347: step 312, loss 1.2268, acc 0.40625\n",
      "2018-03-19T07:20:30.951709: step 313, loss 1.21102, acc 0.40625\n",
      "2018-03-19T07:20:31.435705: step 314, loss 1.26282, acc 0.3125\n",
      "2018-03-19T07:20:31.911121: step 315, loss 1.1346, acc 0.421875\n",
      "2018-03-19T07:20:32.794870: step 316, loss 1.04039, acc 0.546875\n",
      "2018-03-19T07:20:33.345163: step 317, loss 1.29463, acc 0.375\n",
      "2018-03-19T07:20:33.849166: step 318, loss 1.4002, acc 0.4375\n",
      "2018-03-19T07:20:34.363633: step 319, loss 1.41494, acc 0.4375\n",
      "2018-03-19T07:20:34.878501: step 320, loss 1.33766, acc 0.421875\n",
      "2018-03-19T07:20:35.589871: step 321, loss 1.43796, acc 0.28125\n",
      "2018-03-19T07:20:35.991730: step 322, loss 1.06721, acc 0.527273\n",
      "2018-03-19T07:20:36.471047: step 323, loss 0.928411, acc 0.609375\n",
      "2018-03-19T07:20:36.995905: step 324, loss 1.20404, acc 0.46875\n",
      "2018-03-19T07:20:37.550602: step 325, loss 1.20149, acc 0.453125\n",
      "2018-03-19T07:20:38.015017: step 326, loss 1.02796, acc 0.609375\n",
      "2018-03-19T07:20:38.537276: step 327, loss 1.17024, acc 0.40625\n",
      "2018-03-19T07:20:39.406543: step 328, loss 1.12825, acc 0.5\n",
      "2018-03-19T07:20:39.997545: step 329, loss 1.21104, acc 0.4375\n",
      "2018-03-19T07:20:40.709722: step 330, loss 0.867113, acc 0.5\n",
      "2018-03-19T07:20:41.440936: step 331, loss 1.10305, acc 0.484375\n",
      "2018-03-19T07:20:42.248177: step 332, loss 1.09842, acc 0.4375\n",
      "2018-03-19T07:20:42.835767: step 333, loss 1.1539, acc 0.5\n",
      "2018-03-19T07:20:43.412050: step 334, loss 0.927826, acc 0.59375\n",
      "2018-03-19T07:20:44.019640: step 335, loss 1.25346, acc 0.40625\n",
      "2018-03-19T07:20:44.725909: step 336, loss 1.28566, acc 0.4375\n",
      "2018-03-19T07:20:45.402882: step 337, loss 1.23794, acc 0.5625\n",
      "2018-03-19T07:20:46.023800: step 338, loss 1.27507, acc 0.375\n",
      "2018-03-19T07:20:46.627397: step 339, loss 1.18589, acc 0.578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-19T07:20:47.258173: step 340, loss 1.06754, acc 0.515625\n",
      "2018-03-19T07:20:47.847099: step 341, loss 1.20449, acc 0.421875\n",
      "2018-03-19T07:20:48.433540: step 342, loss 0.968376, acc 0.546875\n",
      "2018-03-19T07:20:49.003527: step 343, loss 1.03139, acc 0.53125\n",
      "2018-03-19T07:20:49.591666: step 344, loss 1.13274, acc 0.484375\n",
      "2018-03-19T07:20:50.231172: step 345, loss 1.20584, acc 0.40625\n",
      "2018-03-19T07:20:50.791406: step 346, loss 1.20376, acc 0.484375\n",
      "2018-03-19T07:20:51.557271: step 347, loss 0.950345, acc 0.5\n",
      "2018-03-19T07:20:52.221001: step 348, loss 1.08113, acc 0.515625\n",
      "2018-03-19T07:20:52.836069: step 349, loss 0.945372, acc 0.625\n",
      "2018-03-19T07:20:53.640776: step 350, loss 1.13923, acc 0.484375\n",
      "2018-03-19T07:20:54.235947: step 351, loss 1.04128, acc 0.53125\n",
      "2018-03-19T07:20:55.129844: step 352, loss 1.19931, acc 0.4375\n",
      "2018-03-19T07:20:55.941654: step 353, loss 1.03074, acc 0.46875\n",
      "2018-03-19T07:20:56.655459: step 354, loss 0.986941, acc 0.546875\n",
      "2018-03-19T07:20:57.246628: step 355, loss 0.993438, acc 0.515625\n",
      "2018-03-19T07:20:57.694525: step 356, loss 1.03123, acc 0.40625\n",
      "2018-03-19T07:20:58.181576: step 357, loss 1.14621, acc 0.46875\n",
      "2018-03-19T07:20:58.682306: step 358, loss 0.950508, acc 0.484375\n",
      "2018-03-19T07:20:59.218962: step 359, loss 1.33597, acc 0.46875\n",
      "2018-03-19T07:20:59.752680: step 360, loss 0.990064, acc 0.59375\n",
      "2018-03-19T07:21:00.212642: step 361, loss 1.08953, acc 0.578125\n",
      "2018-03-19T07:21:00.647088: step 362, loss 1.11172, acc 0.46875\n",
      "2018-03-19T07:21:01.100871: step 363, loss 1.19473, acc 0.359375\n",
      "2018-03-19T07:21:01.558627: step 364, loss 1.07025, acc 0.5\n",
      "2018-03-19T07:21:02.005716: step 365, loss 1.12374, acc 0.40625\n",
      "2018-03-19T07:21:02.474489: step 366, loss 1.12183, acc 0.453125\n",
      "2018-03-19T07:21:02.979387: step 367, loss 1.14898, acc 0.4375\n",
      "2018-03-19T07:21:03.432686: step 368, loss 1.06408, acc 0.515625\n",
      "2018-03-19T07:21:03.888511: step 369, loss 1.15932, acc 0.40625\n",
      "2018-03-19T07:21:04.339582: step 370, loss 1.34339, acc 0.375\n",
      "2018-03-19T07:21:04.840494: step 371, loss 1.01785, acc 0.484375\n",
      "2018-03-19T07:21:05.306300: step 372, loss 1.26562, acc 0.421875\n",
      "2018-03-19T07:21:05.832487: step 373, loss 1.12095, acc 0.4375\n",
      "2018-03-19T07:21:06.292997: step 374, loss 1.09491, acc 0.453125\n",
      "2018-03-19T07:21:06.747480: step 375, loss 1.00976, acc 0.453125\n",
      "2018-03-19T07:21:07.267367: step 376, loss 1.09007, acc 0.484375\n",
      "2018-03-19T07:21:07.776484: step 377, loss 1.15247, acc 0.484375\n",
      "2018-03-19T07:21:08.310345: step 378, loss 1.0436, acc 0.4375\n",
      "2018-03-19T07:21:08.821587: step 379, loss 1.10183, acc 0.484375\n",
      "2018-03-19T07:21:09.356808: step 380, loss 1.12222, acc 0.5\n",
      "2018-03-19T07:21:09.836044: step 381, loss 1.06509, acc 0.5\n",
      "2018-03-19T07:21:10.371966: step 382, loss 0.962484, acc 0.546875\n",
      "2018-03-19T07:21:11.062801: step 383, loss 1.29549, acc 0.390625\n",
      "2018-03-19T07:21:11.721776: step 384, loss 1.14825, acc 0.515625\n",
      "2018-03-19T07:21:12.180179: step 385, loss 0.859355, acc 0.609375\n",
      "2018-03-19T07:21:12.649848: step 386, loss 1.03738, acc 0.546875\n",
      "2018-03-19T07:21:13.180254: step 387, loss 1.17095, acc 0.4375\n",
      "2018-03-19T07:21:13.703216: step 388, loss 1.00929, acc 0.515625\n",
      "2018-03-19T07:21:14.163552: step 389, loss 1.12283, acc 0.515625\n",
      "2018-03-19T07:21:14.611339: step 390, loss 1.15326, acc 0.421875\n",
      "2018-03-19T07:21:15.125783: step 391, loss 1.10563, acc 0.40625\n",
      "2018-03-19T07:21:15.575496: step 392, loss 1.24206, acc 0.375\n",
      "2018-03-19T07:21:16.018023: step 393, loss 1.00311, acc 0.484375\n",
      "2018-03-19T07:21:16.494776: step 394, loss 1.11366, acc 0.546875\n",
      "2018-03-19T07:21:17.023116: step 395, loss 1.17455, acc 0.484375\n",
      "2018-03-19T07:21:17.560198: step 396, loss 1.26118, acc 0.4375\n",
      "2018-03-19T07:21:18.022812: step 397, loss 1.20962, acc 0.53125\n",
      "2018-03-19T07:21:18.518522: step 398, loss 1.12641, acc 0.5\n",
      "2018-03-19T07:21:19.015138: step 399, loss 1.16169, acc 0.484375\n",
      "2018-03-19T07:21:19.465339: step 400, loss 1.06933, acc 0.53125\n",
      "\n",
      "Evaluation:\n",
      "2018-03-19T07:21:21.977290: step 400, loss 1.0866, acc 0.406824\n",
      "\n",
      "Saved model checkpoint to /Users/kwheatley/Desktop/w266_nfl/cnn-text-classification-tf/runs/1521461861/checkpoints/model-400\n",
      "\n",
      "2018-03-19T07:21:23.168824: step 401, loss 1.3118, acc 0.515625\n",
      "2018-03-19T07:21:23.609022: step 402, loss 1.03786, acc 0.515625\n",
      "2018-03-19T07:21:24.091664: step 403, loss 1.0362, acc 0.453125\n",
      "2018-03-19T07:21:24.606951: step 404, loss 1.09719, acc 0.5\n",
      "2018-03-19T07:21:25.094928: step 405, loss 1.13201, acc 0.4375\n",
      "2018-03-19T07:21:25.574226: step 406, loss 1.10551, acc 0.453125\n",
      "2018-03-19T07:21:26.042763: step 407, loss 1.23675, acc 0.375\n",
      "2018-03-19T07:21:26.510820: step 408, loss 1.04614, acc 0.46875\n",
      "2018-03-19T07:21:27.090017: step 409, loss 1.26801, acc 0.453125\n",
      "2018-03-19T07:21:27.666656: step 410, loss 1.17534, acc 0.375\n",
      "2018-03-19T07:21:28.164761: step 411, loss 1.1451, acc 0.359375\n",
      "2018-03-19T07:21:28.632866: step 412, loss 1.11847, acc 0.421875\n",
      "2018-03-19T07:21:29.123227: step 413, loss 1.21432, acc 0.390625\n",
      "2018-03-19T07:21:29.615818: step 414, loss 1.09468, acc 0.515625\n",
      "2018-03-19T07:21:30.105785: step 415, loss 1.13312, acc 0.546875\n",
      "2018-03-19T07:21:30.580625: step 416, loss 1.03117, acc 0.46875\n",
      "2018-03-19T07:21:31.075380: step 417, loss 1.11478, acc 0.375\n",
      "2018-03-19T07:21:31.644740: step 418, loss 0.956809, acc 0.515625\n",
      "2018-03-19T07:21:32.093518: step 419, loss 0.95032, acc 0.53125\n",
      "2018-03-19T07:21:32.545841: step 420, loss 1.11128, acc 0.5625\n",
      "2018-03-19T07:21:33.047379: step 421, loss 1.12225, acc 0.421875\n",
      "2018-03-19T07:21:33.532102: step 422, loss 1.22897, acc 0.40625\n",
      "2018-03-19T07:21:34.052547: step 423, loss 1.05062, acc 0.53125\n",
      "2018-03-19T07:21:34.531541: step 424, loss 0.980313, acc 0.578125\n",
      "2018-03-19T07:21:34.977324: step 425, loss 1.12746, acc 0.390625\n",
      "2018-03-19T07:21:35.489074: step 426, loss 1.1263, acc 0.4375\n",
      "2018-03-19T07:21:35.928442: step 427, loss 1.09974, acc 0.5\n",
      "2018-03-19T07:21:36.374604: step 428, loss 1.16648, acc 0.34375\n",
      "2018-03-19T07:21:36.825753: step 429, loss 1.05038, acc 0.453125\n",
      "2018-03-19T07:21:37.262440: step 430, loss 1.17882, acc 0.4375\n",
      "2018-03-19T07:21:37.748322: step 431, loss 1.0015, acc 0.515625\n",
      "2018-03-19T07:21:38.225842: step 432, loss 0.953102, acc 0.5625\n",
      "2018-03-19T07:21:38.709402: step 433, loss 1.22609, acc 0.46875\n",
      "2018-03-19T07:21:39.251517: step 434, loss 1.31315, acc 0.4375\n",
      "2018-03-19T07:21:39.764616: step 435, loss 1.24237, acc 0.3125\n",
      "2018-03-19T07:21:40.234778: step 436, loss 0.937407, acc 0.609375\n",
      "2018-03-19T07:21:40.728931: step 437, loss 1.00031, acc 0.453125\n",
      "2018-03-19T07:21:41.221349: step 438, loss 1.02744, acc 0.546875\n",
      "2018-03-19T07:21:41.674692: step 439, loss 0.985154, acc 0.515625\n",
      "2018-03-19T07:21:42.128050: step 440, loss 1.08849, acc 0.40625\n",
      "2018-03-19T07:21:42.603376: step 441, loss 1.04386, acc 0.515625\n",
      "2018-03-19T07:21:43.055269: step 442, loss 1.03779, acc 0.484375\n",
      "2018-03-19T07:21:43.518232: step 443, loss 1.01611, acc 0.625\n",
      "2018-03-19T07:21:44.014403: step 444, loss 1.1663, acc 0.4375\n",
      "2018-03-19T07:21:44.467657: step 445, loss 1.24781, acc 0.328125\n",
      "2018-03-19T07:21:45.010748: step 446, loss 1.08577, acc 0.5\n",
      "2018-03-19T07:21:45.526399: step 447, loss 1.05389, acc 0.4375\n",
      "2018-03-19T07:21:45.974084: step 448, loss 1.11569, acc 0.484375\n",
      "2018-03-19T07:21:46.427543: step 449, loss 1.03018, acc 0.453125\n",
      "2018-03-19T07:21:46.915039: step 450, loss 1.06658, acc 0.375\n",
      "2018-03-19T07:21:47.362847: step 451, loss 1.13337, acc 0.453125\n",
      "2018-03-19T07:21:47.839938: step 452, loss 1.10937, acc 0.375\n",
      "2018-03-19T07:21:48.342484: step 453, loss 1.07686, acc 0.375\n",
      "2018-03-19T07:21:48.817272: step 454, loss 1.17404, acc 0.46875\n",
      "2018-03-19T07:21:49.310688: step 455, loss 1.18723, acc 0.390625\n",
      "2018-03-19T07:21:49.762649: step 456, loss 1.18283, acc 0.4375\n",
      "2018-03-19T07:21:50.244420: step 457, loss 1.11382, acc 0.4375\n",
      "2018-03-19T07:21:50.734373: step 458, loss 1.15223, acc 0.390625\n",
      "2018-03-19T07:21:51.186993: step 459, loss 1.22202, acc 0.46875\n",
      "2018-03-19T07:21:51.630480: step 460, loss 1.09673, acc 0.453125\n",
      "2018-03-19T07:21:52.084124: step 461, loss 1.08416, acc 0.390625\n",
      "2018-03-19T07:21:52.527832: step 462, loss 1.08714, acc 0.4375\n",
      "2018-03-19T07:21:52.974678: step 463, loss 0.961144, acc 0.578125\n",
      "2018-03-19T07:21:53.436376: step 464, loss 1.15077, acc 0.40625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-19T07:21:53.889970: step 465, loss 0.9485, acc 0.546875\n",
      "2018-03-19T07:21:54.405383: step 466, loss 1.11538, acc 0.4375\n",
      "2018-03-19T07:21:55.034483: step 467, loss 1.04952, acc 0.53125\n",
      "2018-03-19T07:21:55.502721: step 468, loss 0.999691, acc 0.46875\n",
      "2018-03-19T07:21:55.978723: step 469, loss 1.08858, acc 0.359375\n",
      "2018-03-19T07:21:56.448383: step 470, loss 1.10084, acc 0.40625\n",
      "2018-03-19T07:21:56.951507: step 471, loss 1.14486, acc 0.390625\n",
      "2018-03-19T07:21:57.466072: step 472, loss 1.03306, acc 0.453125\n",
      "2018-03-19T07:21:57.961069: step 473, loss 1.16259, acc 0.390625\n",
      "2018-03-19T07:21:58.537816: step 474, loss 1.08232, acc 0.421875\n",
      "2018-03-19T07:21:59.190658: step 475, loss 1.14241, acc 0.3125\n",
      "2018-03-19T07:21:59.721152: step 476, loss 0.979571, acc 0.625\n",
      "2018-03-19T07:22:00.180725: step 477, loss 1.01086, acc 0.46875\n",
      "2018-03-19T07:22:00.693360: step 478, loss 1.05637, acc 0.5\n",
      "2018-03-19T07:22:01.202811: step 479, loss 1.18116, acc 0.4375\n",
      "2018-03-19T07:22:01.674972: step 480, loss 1.05478, acc 0.46875\n",
      "2018-03-19T07:22:02.124850: step 481, loss 1.0353, acc 0.46875\n",
      "2018-03-19T07:22:02.571548: step 482, loss 1.02558, acc 0.5625\n",
      "2018-03-19T07:22:02.967807: step 483, loss 1.2052, acc 0.381818\n",
      "2018-03-19T07:22:03.457255: step 484, loss 0.961355, acc 0.515625\n",
      "2018-03-19T07:22:03.946301: step 485, loss 1.2028, acc 0.453125\n",
      "2018-03-19T07:22:04.438007: step 486, loss 1.0343, acc 0.5\n",
      "2018-03-19T07:22:04.960389: step 487, loss 1.059, acc 0.46875\n",
      "2018-03-19T07:22:05.457553: step 488, loss 0.925484, acc 0.578125\n",
      "2018-03-19T07:22:05.989361: step 489, loss 1.06827, acc 0.5\n",
      "2018-03-19T07:22:06.488977: step 490, loss 1.00142, acc 0.46875\n",
      "2018-03-19T07:22:06.950850: step 491, loss 1.04708, acc 0.453125\n",
      "2018-03-19T07:22:07.446329: step 492, loss 0.927209, acc 0.578125\n",
      "2018-03-19T07:22:07.930293: step 493, loss 1.03455, acc 0.515625\n",
      "2018-03-19T07:22:08.383013: step 494, loss 1.02406, acc 0.4375\n",
      "2018-03-19T07:22:08.819111: step 495, loss 0.87399, acc 0.609375\n",
      "2018-03-19T07:22:09.270705: step 496, loss 0.873973, acc 0.65625\n",
      "2018-03-19T07:22:09.718699: step 497, loss 0.9368, acc 0.53125\n",
      "2018-03-19T07:22:10.213587: step 498, loss 0.962213, acc 0.546875\n",
      "2018-03-19T07:22:10.710532: step 499, loss 0.937561, acc 0.515625\n",
      "2018-03-19T07:22:11.320769: step 500, loss 0.972344, acc 0.609375\n",
      "\n",
      "Evaluation:\n",
      "2018-03-19T07:22:13.795326: step 500, loss 1.09156, acc 0.404199\n",
      "\n",
      "Saved model checkpoint to /Users/kwheatley/Desktop/w266_nfl/cnn-text-classification-tf/runs/1521461861/checkpoints/model-500\n",
      "\n",
      "2018-03-19T07:22:14.946631: step 501, loss 1.07646, acc 0.515625\n",
      "2018-03-19T07:22:15.452581: step 502, loss 1.00726, acc 0.515625\n",
      "2018-03-19T07:22:15.909947: step 503, loss 0.821208, acc 0.703125\n",
      "2018-03-19T07:22:16.442551: step 504, loss 1.05231, acc 0.4375\n",
      "2018-03-19T07:22:16.938145: step 505, loss 1.04083, acc 0.46875\n",
      "2018-03-19T07:22:17.410069: step 506, loss 0.999418, acc 0.546875\n",
      "2018-03-19T07:22:17.891908: step 507, loss 1.15146, acc 0.46875\n",
      "2018-03-19T07:22:18.368361: step 508, loss 1.14492, acc 0.390625\n",
      "2018-03-19T07:22:18.847445: step 509, loss 0.996381, acc 0.578125\n",
      "2018-03-19T07:22:19.356934: step 510, loss 0.984603, acc 0.46875\n",
      "2018-03-19T07:22:19.851884: step 511, loss 0.948606, acc 0.484375\n",
      "2018-03-19T07:22:20.317608: step 512, loss 0.999375, acc 0.53125\n",
      "2018-03-19T07:22:20.760458: step 513, loss 1.05861, acc 0.46875\n",
      "2018-03-19T07:22:21.283855: step 514, loss 0.965936, acc 0.578125\n",
      "2018-03-19T07:22:21.759518: step 515, loss 0.965217, acc 0.59375\n",
      "2018-03-19T07:22:22.263571: step 516, loss 1.085, acc 0.453125\n",
      "2018-03-19T07:22:22.938326: step 517, loss 1.20121, acc 0.375\n",
      "2018-03-19T07:22:23.637306: step 518, loss 0.992303, acc 0.5\n",
      "2018-03-19T07:22:24.079009: step 519, loss 1.11532, acc 0.46875\n",
      "2018-03-19T07:22:24.519729: step 520, loss 1.01708, acc 0.421875\n",
      "2018-03-19T07:22:24.966228: step 521, loss 1.15145, acc 0.34375\n",
      "2018-03-19T07:22:25.459331: step 522, loss 1.05032, acc 0.515625\n",
      "2018-03-19T07:22:26.043442: step 523, loss 1.12854, acc 0.453125\n",
      "2018-03-19T07:22:26.762740: step 524, loss 0.978955, acc 0.59375\n",
      "2018-03-19T07:22:27.300383: step 525, loss 0.919012, acc 0.53125\n",
      "2018-03-19T07:22:27.777289: step 526, loss 1.00845, acc 0.421875\n",
      "2018-03-19T07:22:28.311595: step 527, loss 1.00722, acc 0.53125\n",
      "2018-03-19T07:22:28.782802: step 528, loss 0.977108, acc 0.609375\n",
      "2018-03-19T07:22:29.239462: step 529, loss 0.846443, acc 0.625\n",
      "2018-03-19T07:22:29.691475: step 530, loss 1.00868, acc 0.46875\n",
      "2018-03-19T07:22:30.210072: step 531, loss 1.01489, acc 0.5\n",
      "2018-03-19T07:22:30.782365: step 532, loss 1.02089, acc 0.5\n",
      "2018-03-19T07:22:31.262454: step 533, loss 0.923949, acc 0.59375\n",
      "2018-03-19T07:22:31.762211: step 534, loss 1.01243, acc 0.515625\n",
      "2018-03-19T07:22:32.223247: step 535, loss 1.03038, acc 0.546875\n",
      "2018-03-19T07:22:32.707014: step 536, loss 0.789802, acc 0.65625\n",
      "2018-03-19T07:22:33.183776: step 537, loss 0.989776, acc 0.515625\n",
      "2018-03-19T07:22:33.692412: step 538, loss 1.00865, acc 0.484375\n",
      "2018-03-19T07:22:34.164722: step 539, loss 0.956055, acc 0.59375\n",
      "2018-03-19T07:22:34.659108: step 540, loss 0.869119, acc 0.59375\n",
      "2018-03-19T07:22:35.145790: step 541, loss 1.03081, acc 0.5\n",
      "2018-03-19T07:22:35.595561: step 542, loss 0.808585, acc 0.609375\n",
      "2018-03-19T07:22:36.100576: step 543, loss 1.08841, acc 0.5625\n",
      "2018-03-19T07:22:36.595947: step 544, loss 0.934777, acc 0.53125\n",
      "2018-03-19T07:22:37.116784: step 545, loss 0.847659, acc 0.640625\n",
      "2018-03-19T07:22:37.595759: step 546, loss 0.894359, acc 0.515625\n",
      "2018-03-19T07:22:38.066959: step 547, loss 0.987941, acc 0.546875\n",
      "2018-03-19T07:22:38.531296: step 548, loss 0.967173, acc 0.5\n",
      "2018-03-19T07:22:38.984394: step 549, loss 1.10852, acc 0.40625\n",
      "2018-03-19T07:22:39.419563: step 550, loss 0.941721, acc 0.546875\n",
      "2018-03-19T07:22:39.912289: step 551, loss 0.981408, acc 0.515625\n",
      "2018-03-19T07:22:40.419744: step 552, loss 0.842602, acc 0.640625\n",
      "2018-03-19T07:22:40.882351: step 553, loss 0.997572, acc 0.515625\n",
      "2018-03-19T07:22:41.360465: step 554, loss 1.11613, acc 0.5\n",
      "2018-03-19T07:22:41.826136: step 555, loss 1.12324, acc 0.5\n",
      "2018-03-19T07:22:42.294366: step 556, loss 0.934495, acc 0.546875\n",
      "2018-03-19T07:22:42.792242: step 557, loss 1.01372, acc 0.53125\n",
      "2018-03-19T07:22:43.240413: step 558, loss 1.11625, acc 0.421875\n",
      "2018-03-19T07:22:43.739911: step 559, loss 0.969459, acc 0.53125\n",
      "2018-03-19T07:22:44.240780: step 560, loss 0.977241, acc 0.515625\n",
      "2018-03-19T07:22:44.726781: step 561, loss 1.09429, acc 0.375\n",
      "2018-03-19T07:22:45.231618: step 562, loss 0.901789, acc 0.484375\n",
      "2018-03-19T07:22:45.736795: step 563, loss 0.908802, acc 0.609375\n",
      "2018-03-19T07:22:46.198177: step 564, loss 1.02604, acc 0.578125\n",
      "2018-03-19T07:22:46.667472: step 565, loss 0.936672, acc 0.484375\n",
      "2018-03-19T07:22:47.226392: step 566, loss 0.92091, acc 0.46875\n",
      "2018-03-19T07:22:47.731889: step 567, loss 0.939556, acc 0.53125\n",
      "2018-03-19T07:22:48.237767: step 568, loss 0.910515, acc 0.578125\n",
      "2018-03-19T07:22:48.717001: step 569, loss 1.13788, acc 0.375\n",
      "2018-03-19T07:22:49.184818: step 570, loss 0.864047, acc 0.65625\n",
      "2018-03-19T07:22:49.629461: step 571, loss 0.963927, acc 0.5625\n",
      "2018-03-19T07:22:50.094863: step 572, loss 0.985071, acc 0.640625\n",
      "2018-03-19T07:22:50.701602: step 573, loss 0.884952, acc 0.625\n",
      "2018-03-19T07:22:51.197617: step 574, loss 1.03579, acc 0.46875\n",
      "2018-03-19T07:22:52.074330: step 575, loss 1.13756, acc 0.421875\n",
      "2018-03-19T07:22:52.572249: step 576, loss 0.911083, acc 0.546875\n",
      "2018-03-19T07:22:53.149590: step 577, loss 0.976472, acc 0.46875\n",
      "2018-03-19T07:22:53.627939: step 578, loss 1.0768, acc 0.421875\n",
      "2018-03-19T07:22:54.104268: step 579, loss 1.0951, acc 0.421875\n",
      "2018-03-19T07:22:54.553014: step 580, loss 0.987831, acc 0.53125\n",
      "2018-03-19T07:22:54.994201: step 581, loss 0.948132, acc 0.5625\n",
      "2018-03-19T07:22:55.439563: step 582, loss 0.88936, acc 0.53125\n",
      "2018-03-19T07:22:55.876136: step 583, loss 1.12521, acc 0.46875\n",
      "2018-03-19T07:22:56.311237: step 584, loss 0.989971, acc 0.46875\n",
      "2018-03-19T07:22:56.746011: step 585, loss 0.882109, acc 0.578125\n",
      "2018-03-19T07:22:57.199599: step 586, loss 0.906913, acc 0.625\n",
      "2018-03-19T07:22:57.670239: step 587, loss 1.05221, acc 0.5\n",
      "2018-03-19T07:22:58.138595: step 588, loss 1.08267, acc 0.546875\n",
      "2018-03-19T07:22:58.735486: step 589, loss 1.08281, acc 0.46875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-19T07:22:59.195357: step 590, loss 0.976144, acc 0.546875\n",
      "2018-03-19T07:22:59.762249: step 591, loss 0.878112, acc 0.671875\n",
      "2018-03-19T07:23:00.205624: step 592, loss 1.05281, acc 0.453125\n",
      "2018-03-19T07:23:00.643395: step 593, loss 1.09133, acc 0.453125\n",
      "2018-03-19T07:23:01.093178: step 594, loss 1.20056, acc 0.46875\n",
      "2018-03-19T07:23:01.575012: step 595, loss 0.866691, acc 0.53125\n",
      "2018-03-19T07:23:02.168305: step 596, loss 0.860682, acc 0.5625\n",
      "2018-03-19T07:23:02.789722: step 597, loss 0.935743, acc 0.5625\n",
      "2018-03-19T07:23:03.609932: step 598, loss 0.99508, acc 0.578125\n",
      "2018-03-19T07:23:04.198009: step 599, loss 0.98832, acc 0.59375\n",
      "2018-03-19T07:23:04.840210: step 600, loss 1.07034, acc 0.4375\n",
      "\n",
      "Evaluation:\n",
      "2018-03-19T07:23:07.682663: step 600, loss 1.07312, acc 0.419948\n",
      "\n",
      "Saved model checkpoint to /Users/kwheatley/Desktop/w266_nfl/cnn-text-classification-tf/runs/1521461861/checkpoints/model-600\n",
      "\n",
      "2018-03-19T07:23:08.788546: step 601, loss 1.03516, acc 0.46875\n",
      "2018-03-19T07:23:09.264625: step 602, loss 0.844992, acc 0.640625\n",
      "2018-03-19T07:23:10.006421: step 603, loss 1.13301, acc 0.328125\n",
      "2018-03-19T07:23:10.538926: step 604, loss 1.09063, acc 0.40625\n",
      "2018-03-19T07:23:11.229091: step 605, loss 1.06442, acc 0.453125\n",
      "2018-03-19T07:23:11.866287: step 606, loss 0.997356, acc 0.546875\n",
      "2018-03-19T07:23:12.718810: step 607, loss 0.89346, acc 0.5625\n",
      "2018-03-19T07:23:13.513004: step 608, loss 1.177, acc 0.46875\n",
      "2018-03-19T07:23:14.085760: step 609, loss 0.954736, acc 0.625\n",
      "2018-03-19T07:23:14.570113: step 610, loss 0.968981, acc 0.5625\n",
      "2018-03-19T07:23:15.088066: step 611, loss 0.868732, acc 0.640625\n",
      "2018-03-19T07:23:15.574548: step 612, loss 0.951495, acc 0.546875\n",
      "2018-03-19T07:23:16.012335: step 613, loss 1.05829, acc 0.5625\n",
      "2018-03-19T07:23:16.685879: step 614, loss 1.03507, acc 0.4375\n",
      "2018-03-19T07:23:17.149947: step 615, loss 1.15117, acc 0.421875\n",
      "2018-03-19T07:23:17.609797: step 616, loss 1.02616, acc 0.515625\n",
      "2018-03-19T07:23:18.107161: step 617, loss 0.988455, acc 0.5\n",
      "2018-03-19T07:23:18.599222: step 618, loss 0.899793, acc 0.59375\n",
      "2018-03-19T07:23:19.057684: step 619, loss 1.00177, acc 0.515625\n",
      "2018-03-19T07:23:19.508616: step 620, loss 0.890558, acc 0.578125\n",
      "2018-03-19T07:23:19.950027: step 621, loss 0.91796, acc 0.546875\n",
      "2018-03-19T07:23:20.392735: step 622, loss 1.04753, acc 0.359375\n",
      "2018-03-19T07:23:20.828613: step 623, loss 1.02352, acc 0.515625\n",
      "2018-03-19T07:23:21.276001: step 624, loss 0.941562, acc 0.609375\n",
      "2018-03-19T07:23:21.722940: step 625, loss 0.992165, acc 0.515625\n",
      "2018-03-19T07:23:22.160818: step 626, loss 1.04559, acc 0.53125\n",
      "2018-03-19T07:23:22.606635: step 627, loss 1.06635, acc 0.421875\n",
      "2018-03-19T07:23:23.068269: step 628, loss 1.00797, acc 0.484375\n",
      "2018-03-19T07:23:23.505723: step 629, loss 1.08631, acc 0.4375\n",
      "2018-03-19T07:23:23.940313: step 630, loss 1.06841, acc 0.515625\n",
      "2018-03-19T07:23:24.382529: step 631, loss 1.046, acc 0.46875\n",
      "2018-03-19T07:23:24.835694: step 632, loss 1.1221, acc 0.46875\n",
      "2018-03-19T07:23:25.434442: step 633, loss 0.866398, acc 0.578125\n",
      "2018-03-19T07:23:25.956025: step 634, loss 0.986272, acc 0.53125\n",
      "2018-03-19T07:23:26.400432: step 635, loss 1.2185, acc 0.46875\n",
      "2018-03-19T07:23:26.912436: step 636, loss 0.984083, acc 0.578125\n",
      "2018-03-19T07:23:27.410924: step 637, loss 1.10354, acc 0.5\n",
      "2018-03-19T07:23:27.964575: step 638, loss 1.00877, acc 0.484375\n",
      "2018-03-19T07:23:28.594355: step 639, loss 1.05025, acc 0.4375\n",
      "2018-03-19T07:23:29.142188: step 640, loss 1.04675, acc 0.421875\n",
      "2018-03-19T07:23:29.677899: step 641, loss 0.884691, acc 0.609375\n",
      "2018-03-19T07:23:30.208576: step 642, loss 1.03826, acc 0.515625\n",
      "2018-03-19T07:23:30.978210: step 643, loss 0.979667, acc 0.5625\n",
      "2018-03-19T07:23:31.398073: step 644, loss 1.04325, acc 0.436364\n",
      "2018-03-19T07:23:32.005560: step 645, loss 0.833378, acc 0.6875\n",
      "2018-03-19T07:23:32.577800: step 646, loss 0.968271, acc 0.515625\n",
      "2018-03-19T07:23:33.394257: step 647, loss 0.984094, acc 0.484375\n",
      "2018-03-19T07:23:34.121336: step 648, loss 0.995812, acc 0.5625\n",
      "2018-03-19T07:23:34.720525: step 649, loss 0.976303, acc 0.546875\n",
      "2018-03-19T07:23:35.193680: step 650, loss 0.771133, acc 0.640625\n",
      "2018-03-19T07:23:35.633039: step 651, loss 0.999601, acc 0.53125\n",
      "2018-03-19T07:23:36.075229: step 652, loss 0.794288, acc 0.59375\n",
      "2018-03-19T07:23:36.516711: step 653, loss 0.945188, acc 0.515625\n",
      "2018-03-19T07:23:36.959699: step 654, loss 0.931836, acc 0.625\n",
      "2018-03-19T07:23:37.403855: step 655, loss 1.01701, acc 0.53125\n",
      "2018-03-19T07:23:37.889821: step 656, loss 0.99231, acc 0.546875\n",
      "2018-03-19T07:23:38.329169: step 657, loss 0.907988, acc 0.59375\n",
      "2018-03-19T07:23:38.771167: step 658, loss 0.748779, acc 0.75\n",
      "2018-03-19T07:23:39.236701: step 659, loss 0.910038, acc 0.5625\n",
      "2018-03-19T07:23:39.809327: step 660, loss 0.910948, acc 0.5625\n",
      "2018-03-19T07:23:40.344066: step 661, loss 1.01818, acc 0.5625\n",
      "2018-03-19T07:23:40.969809: step 662, loss 0.897044, acc 0.5625\n",
      "2018-03-19T07:23:41.757923: step 663, loss 1.02725, acc 0.5\n",
      "2018-03-19T07:23:42.208983: step 664, loss 1.02594, acc 0.46875\n",
      "2018-03-19T07:23:42.998206: step 665, loss 0.899169, acc 0.609375\n",
      "2018-03-19T07:23:43.536750: step 666, loss 0.934022, acc 0.578125\n",
      "2018-03-19T07:23:44.178610: step 667, loss 0.842633, acc 0.5625\n",
      "2018-03-19T07:23:44.661722: step 668, loss 0.97982, acc 0.5625\n",
      "2018-03-19T07:23:45.221152: step 669, loss 0.907218, acc 0.53125\n",
      "2018-03-19T07:23:45.807317: step 670, loss 0.972938, acc 0.453125\n",
      "2018-03-19T07:23:46.252131: step 671, loss 0.996101, acc 0.5625\n",
      "2018-03-19T07:23:46.710196: step 672, loss 0.873323, acc 0.59375\n",
      "2018-03-19T07:23:47.185714: step 673, loss 0.857413, acc 0.546875\n",
      "2018-03-19T07:23:47.648827: step 674, loss 0.998548, acc 0.53125\n",
      "2018-03-19T07:23:48.119877: step 675, loss 0.847841, acc 0.59375\n",
      "2018-03-19T07:23:48.566827: step 676, loss 0.867357, acc 0.5625\n",
      "2018-03-19T07:23:49.015719: step 677, loss 0.981674, acc 0.515625\n",
      "2018-03-19T07:23:49.648442: step 678, loss 0.952977, acc 0.5\n",
      "2018-03-19T07:23:50.201375: step 679, loss 0.889927, acc 0.6875\n",
      "2018-03-19T07:23:50.833060: step 680, loss 0.986178, acc 0.546875\n",
      "2018-03-19T07:23:51.517497: step 681, loss 0.836108, acc 0.59375\n",
      "2018-03-19T07:23:52.082274: step 682, loss 1.05121, acc 0.4375\n",
      "2018-03-19T07:23:52.551327: step 683, loss 1.03499, acc 0.484375\n",
      "2018-03-19T07:23:53.044901: step 684, loss 0.910531, acc 0.546875\n",
      "2018-03-19T07:23:53.632372: step 685, loss 1.11644, acc 0.453125\n",
      "2018-03-19T07:23:54.164994: step 686, loss 0.800388, acc 0.609375\n",
      "2018-03-19T07:23:54.660596: step 687, loss 0.938476, acc 0.53125\n",
      "2018-03-19T07:23:55.268185: step 688, loss 0.894902, acc 0.640625\n",
      "2018-03-19T07:23:55.835421: step 689, loss 0.979589, acc 0.546875\n",
      "2018-03-19T07:23:56.326444: step 690, loss 0.960376, acc 0.53125\n",
      "2018-03-19T07:23:56.932267: step 691, loss 0.981185, acc 0.578125\n",
      "2018-03-19T07:23:57.499135: step 692, loss 0.869081, acc 0.59375\n",
      "2018-03-19T07:23:58.024195: step 693, loss 0.898095, acc 0.640625\n",
      "2018-03-19T07:23:58.688062: step 694, loss 1.00977, acc 0.453125\n",
      "2018-03-19T07:23:59.229974: step 695, loss 0.865379, acc 0.578125\n",
      "2018-03-19T07:23:59.665340: step 696, loss 1.01208, acc 0.5\n",
      "2018-03-19T07:24:00.167366: step 697, loss 0.947068, acc 0.59375\n",
      "2018-03-19T07:24:00.625776: step 698, loss 0.87009, acc 0.609375\n",
      "2018-03-19T07:24:01.250993: step 699, loss 0.912722, acc 0.5625\n",
      "2018-03-19T07:24:01.807714: step 700, loss 0.933751, acc 0.515625\n",
      "\n",
      "Evaluation:\n",
      "2018-03-19T07:24:04.498272: step 700, loss 1.08013, acc 0.427822\n",
      "\n",
      "Saved model checkpoint to /Users/kwheatley/Desktop/w266_nfl/cnn-text-classification-tf/runs/1521461861/checkpoints/model-700\n",
      "\n",
      "2018-03-19T07:24:05.993622: step 701, loss 0.929068, acc 0.5625\n",
      "2018-03-19T07:24:06.472061: step 702, loss 0.955478, acc 0.625\n",
      "2018-03-19T07:24:07.060939: step 703, loss 1.00004, acc 0.515625\n",
      "2018-03-19T07:24:07.615898: step 704, loss 1.00856, acc 0.546875\n",
      "2018-03-19T07:24:08.497744: step 705, loss 0.832034, acc 0.625\n",
      "2018-03-19T07:24:09.075048: step 706, loss 0.800089, acc 0.65625\n",
      "2018-03-19T07:24:09.527437: step 707, loss 1.01989, acc 0.46875\n",
      "2018-03-19T07:24:10.138606: step 708, loss 0.968164, acc 0.46875\n",
      "2018-03-19T07:24:10.664706: step 709, loss 0.90678, acc 0.4375\n",
      "2018-03-19T07:24:11.192593: step 710, loss 0.936721, acc 0.546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-19T07:24:11.672567: step 711, loss 0.851408, acc 0.609375\n",
      "2018-03-19T07:24:12.305492: step 712, loss 0.97621, acc 0.515625\n",
      "2018-03-19T07:24:12.802431: step 713, loss 0.899176, acc 0.53125\n",
      "2018-03-19T07:24:13.262804: step 714, loss 0.907703, acc 0.578125\n",
      "2018-03-19T07:24:13.721444: step 715, loss 0.880114, acc 0.625\n",
      "2018-03-19T07:24:14.164752: step 716, loss 0.918363, acc 0.53125\n",
      "2018-03-19T07:24:14.611972: step 717, loss 0.892787, acc 0.515625\n",
      "2018-03-19T07:24:15.159019: step 718, loss 0.926338, acc 0.53125\n",
      "2018-03-19T07:24:15.827879: step 719, loss 1.00134, acc 0.578125\n",
      "2018-03-19T07:24:16.428266: step 720, loss 0.895541, acc 0.546875\n",
      "2018-03-19T07:24:17.079491: step 721, loss 0.890483, acc 0.65625\n",
      "2018-03-19T07:24:17.858338: step 722, loss 0.915272, acc 0.625\n",
      "2018-03-19T07:24:18.477505: step 723, loss 0.988623, acc 0.515625\n",
      "2018-03-19T07:24:19.014238: step 724, loss 1.0209, acc 0.515625\n",
      "2018-03-19T07:24:19.487490: step 725, loss 0.987526, acc 0.53125\n",
      "2018-03-19T07:24:19.941773: step 726, loss 0.904672, acc 0.5\n",
      "2018-03-19T07:24:20.377893: step 727, loss 0.97941, acc 0.484375\n",
      "2018-03-19T07:24:20.824521: step 728, loss 0.990841, acc 0.5\n",
      "2018-03-19T07:24:21.263066: step 729, loss 0.924035, acc 0.609375\n",
      "2018-03-19T07:24:21.694646: step 730, loss 1.01392, acc 0.5\n",
      "2018-03-19T07:24:22.134457: step 731, loss 0.841973, acc 0.578125\n",
      "2018-03-19T07:24:22.571864: step 732, loss 0.939015, acc 0.546875\n",
      "2018-03-19T07:24:23.017161: step 733, loss 0.879912, acc 0.578125\n",
      "2018-03-19T07:24:23.519127: step 734, loss 0.847655, acc 0.640625\n",
      "2018-03-19T07:24:24.020476: step 735, loss 0.806169, acc 0.640625\n",
      "2018-03-19T07:24:24.487357: step 736, loss 0.937115, acc 0.609375\n",
      "2018-03-19T07:24:24.973082: step 737, loss 0.845773, acc 0.609375\n",
      "2018-03-19T07:24:25.445872: step 738, loss 0.797098, acc 0.640625\n",
      "2018-03-19T07:24:25.881175: step 739, loss 1.07728, acc 0.515625\n",
      "2018-03-19T07:24:26.356992: step 740, loss 0.977492, acc 0.515625\n",
      "2018-03-19T07:24:26.804017: step 741, loss 0.950844, acc 0.5\n",
      "2018-03-19T07:24:27.573795: step 742, loss 0.884647, acc 0.640625\n",
      "2018-03-19T07:24:28.024209: step 743, loss 0.982678, acc 0.53125\n",
      "2018-03-19T07:24:28.461058: step 744, loss 0.911922, acc 0.546875\n",
      "2018-03-19T07:24:28.896876: step 745, loss 0.877764, acc 0.6875\n",
      "2018-03-19T07:24:29.346716: step 746, loss 0.983914, acc 0.484375\n",
      "2018-03-19T07:24:29.789841: step 747, loss 0.834128, acc 0.71875\n",
      "2018-03-19T07:24:30.328033: step 748, loss 0.957922, acc 0.546875\n",
      "2018-03-19T07:24:30.827085: step 749, loss 0.984859, acc 0.46875\n",
      "2018-03-19T07:24:31.316084: step 750, loss 1.0378, acc 0.53125\n",
      "2018-03-19T07:24:31.772969: step 751, loss 0.934669, acc 0.578125\n",
      "2018-03-19T07:24:32.258115: step 752, loss 0.980584, acc 0.53125\n",
      "2018-03-19T07:24:32.724453: step 753, loss 0.867887, acc 0.578125\n",
      "2018-03-19T07:24:33.481164: step 754, loss 0.882658, acc 0.640625\n",
      "2018-03-19T07:24:34.110074: step 755, loss 0.922588, acc 0.65625\n",
      "2018-03-19T07:24:34.606139: step 756, loss 0.866225, acc 0.640625\n",
      "2018-03-19T07:24:35.123661: step 757, loss 0.932793, acc 0.53125\n",
      "2018-03-19T07:24:35.631732: step 758, loss 0.963747, acc 0.5625\n",
      "2018-03-19T07:24:36.207668: step 759, loss 0.874329, acc 0.53125\n",
      "2018-03-19T07:24:36.754973: step 760, loss 0.769503, acc 0.671875\n",
      "2018-03-19T07:24:37.230219: step 761, loss 0.962683, acc 0.578125\n",
      "2018-03-19T07:24:37.938953: step 762, loss 0.823057, acc 0.625\n",
      "2018-03-19T07:24:38.630498: step 763, loss 0.994644, acc 0.515625\n",
      "2018-03-19T07:24:39.458332: step 764, loss 0.969968, acc 0.515625\n",
      "2018-03-19T07:24:40.385577: step 765, loss 0.989144, acc 0.515625\n",
      "2018-03-19T07:24:41.028995: step 766, loss 0.987789, acc 0.453125\n",
      "2018-03-19T07:24:41.726549: step 767, loss 1.07444, acc 0.484375\n",
      "2018-03-19T07:24:42.404914: step 768, loss 0.921021, acc 0.546875\n",
      "2018-03-19T07:24:43.006102: step 769, loss 0.887697, acc 0.640625\n",
      "2018-03-19T07:24:43.576164: step 770, loss 0.977042, acc 0.59375\n",
      "2018-03-19T07:24:44.026852: step 771, loss 0.893049, acc 0.53125\n",
      "2018-03-19T07:24:44.606144: step 772, loss 1.01148, acc 0.5\n",
      "2018-03-19T07:24:45.440952: step 773, loss 0.864429, acc 0.609375\n",
      "2018-03-19T07:24:45.994811: step 774, loss 0.943749, acc 0.5\n",
      "2018-03-19T07:24:46.577605: step 775, loss 0.969109, acc 0.59375\n",
      "2018-03-19T07:24:47.226592: step 776, loss 0.887252, acc 0.5625\n",
      "2018-03-19T07:24:47.976179: step 777, loss 0.871462, acc 0.609375\n",
      "2018-03-19T07:24:48.774550: step 778, loss 1.09353, acc 0.453125\n",
      "2018-03-19T07:24:49.391866: step 779, loss 0.915429, acc 0.609375\n",
      "2018-03-19T07:24:49.997428: step 780, loss 0.992819, acc 0.484375\n",
      "2018-03-19T07:24:50.519965: step 781, loss 0.925082, acc 0.609375\n",
      "2018-03-19T07:24:50.997610: step 782, loss 0.939711, acc 0.546875\n",
      "2018-03-19T07:24:51.461990: step 783, loss 0.861625, acc 0.65625\n",
      "2018-03-19T07:24:51.941517: step 784, loss 0.859313, acc 0.65625\n",
      "2018-03-19T07:24:52.432369: step 785, loss 0.9569, acc 0.515625\n",
      "2018-03-19T07:24:52.893202: step 786, loss 1.05257, acc 0.46875\n",
      "2018-03-19T07:24:53.347044: step 787, loss 0.834295, acc 0.59375\n",
      "2018-03-19T07:24:53.782215: step 788, loss 0.930131, acc 0.625\n",
      "2018-03-19T07:24:54.288600: step 789, loss 0.964504, acc 0.578125\n",
      "2018-03-19T07:24:54.811651: step 790, loss 0.900083, acc 0.546875\n",
      "2018-03-19T07:24:55.307840: step 791, loss 1.00714, acc 0.609375\n",
      "2018-03-19T07:24:55.752052: step 792, loss 0.940528, acc 0.53125\n",
      "2018-03-19T07:24:56.275050: step 793, loss 0.937829, acc 0.5\n",
      "2018-03-19T07:24:56.874100: step 794, loss 1.02342, acc 0.515625\n",
      "2018-03-19T07:24:57.481792: step 795, loss 1.08492, acc 0.4375\n",
      "2018-03-19T07:24:58.124192: step 796, loss 1.09532, acc 0.484375\n",
      "2018-03-19T07:24:58.766685: step 797, loss 1.07936, acc 0.515625\n",
      "2018-03-19T07:24:59.464818: step 798, loss 1.02965, acc 0.5625\n",
      "2018-03-19T07:25:00.112418: step 799, loss 0.967753, acc 0.515625\n",
      "2018-03-19T07:25:00.720379: step 800, loss 0.957142, acc 0.546875\n",
      "\n",
      "Evaluation:\n",
      "2018-03-19T07:25:04.537259: step 800, loss 1.06182, acc 0.415573\n",
      "\n",
      "Saved model checkpoint to /Users/kwheatley/Desktop/w266_nfl/cnn-text-classification-tf/runs/1521461861/checkpoints/model-800\n",
      "\n",
      "2018-03-19T07:25:05.910825: step 801, loss 0.826413, acc 0.625\n",
      "2018-03-19T07:25:06.380326: step 802, loss 1.11718, acc 0.484375\n",
      "2018-03-19T07:25:06.898967: step 803, loss 1.02363, acc 0.53125\n",
      "2018-03-19T07:25:07.448112: step 804, loss 0.925091, acc 0.4375\n",
      "2018-03-19T07:25:07.863353: step 805, loss 1.02406, acc 0.527273\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=embedding_dim,\n",
    "            filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
    "            num_filters=num_filters,\n",
    "            l2_reg_lambda=l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python36]",
   "language": "python",
   "name": "conda-env-python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
